[{"content":"本文旨在回顾我这三个月准备考研的经历。无论结果如何，我想给自己留下一份奋斗的记录，如果能帮到后来者那就更好了。\n背景 在五月份的时候，我知道我处在保研排名的边缘，有一定可能获取不到推免资格，于是我在当时购买了几本参考书，并且复习了一小段时间的高等数学。 但是随着夏令营、预推免和毕业实习等工作的推进，我动摇了，一度认为自己真的能成功保研，完全将复习考研抛之脑后。 等到我反应过来的时候，已经是2023年9月18日，学院公布推免指标分配方案的时候，其中我的专业的推免指标仅为同学院其余两个专业的二分之一与三分之一（总人数相近），远低于我的预期。 以公布的名额数量来算，推免名额轮不到我，这时我才清醒——保研这条路我肯定是走不通了。\n考研是12月23日，当时已经是9月18日，也就是说留给我的复习时间仅有三个月，如何合理安排复习时间，最大化我的考研分数，便是我当时需要解决的问题。\n调查阶段 调整心态 无法保研的信息让当时的我陷入了绝望，感觉已经无路可走了——因为在当时的我看来考研真的好难，而我又没有进入工作的心理准备（更何况还是在互联网寒冬的当下），出国对家里造成的经济负担又太大了。 我当时一度有过轻生的念头，好在我没有任何实施它的勇气，这时调整好心态，重新振作起来是最重要的。 在家人和朋友们的帮助下，我逐渐看开了，意识到考研可能没有想象中的那样艰难，于是确立了考研的目标。\n确定目标 在努力复习的同时，选择一个好的目标院校同样重要，因为10月初就要报名了。 别的考研同学可以通过之前几个月的复习时间找准自己的定位，从而决定好一个匹配自己水平的学校。 相较于他们，我只能在两周的时间内确定一个我通过仅仅三个月的复习时间有较大把握考上的学校和专业。\n首先排除本校，经过保研这事我已经不想在本校待着了（更何况学校挺热门的……不好考）。 在用爬虫抓取了研招网的专业目录并在本地筛选后，我筛选出了几个目标，包括北京邮电大学、东南大学（简称东大）等。 考虑到我的复习时间和之前预推免的情况，在与家人沟通、参考了网络上一些在读或毕业学长学姐的经验帖后，我最终选择了相对比较好考的东南大学-蒙纳士大学苏州联合研究生院（简称东蒙）。\n好考是有原因的——因为是中外合办的学院，学费相较于别的学校和学院贵很多（6.2万元/年） [1] ，所以竞争对手相对较少，往年的分数线低 [2] ，报考-录取比也较低 [3] 。\n分析现状 东蒙电子信息专硕的考试科目是 101 思想政治理论、201 英语（一）、301 数学（一）和 935 计算机专业基础（东南大学自命题）。 935的考纲会在东大计算机学院官网发布 [4] ，包括操作系统、计算机组成原理、数据结构，其中操作系统占比最大。\n知道了要考什么，接下来就要分析现状了，依此分配复习时间。\n我当时的状况是：高数只看完极限和一元函数的微分学，其余学科完全没复习。 我的优势是英语不错：六级分数 605，英语一真题的选择题大多能在一个小时内做完，且扣分在8分以内，所以几乎不用太多时间复习英语。 935要考的三门我掌握的也不错，只需要回顾一下知识点，做一些题即可。 我的劣势是我背书很难背下来，因此以背书为主的政治于我而言较为困难。\n复习 我的基础复习阶段大概是9月20日-11月中旬，大约两个月时间。因为复习的时间有限，各科的复习是同时进行的。 强化和冲刺阶段是11月中旬一直到考试前。\n数学（一） 数学一分为三个子科目：高等数学、线性代数、概率论与数理统计，占比最大的是高等数学，因此在复习数学时要着重准备高等数学。\n在同学的推荐下，我高数部分跟的是张宇的课，张宇的优点是在基础阶段讲的很全面，也会顺带着讲一些技巧，适合我这种时间紧张的人；线性代数部分最开始跟的是李永乐，但是没听下去，后来就转汤家凤了；概率论部分没有看课，只是看书回忆自己大一学的东西。在看课的同时，做题巩固也很重要，我的节奏是看完一章的课就把《张宇 1000 题》对应的章节的A、B、C部分做完，C部分的题做不出来的暂时也不用深究。\n在强化阶段主要写之前不会写的C组题，C组题做完了就做真题，同时看重点/常考知识点的强化课程（来不及全看完了），例如高等数学的中值定理、无穷级数、线性代数的二次型等。\n冲刺阶段接着做真题，以及张宇的8套预测卷和4套终极预测卷。\n参考书目：张宇基础 30 讲、张宇 1000 题、数学一真题、张宇8套预测卷、张宇4套终极预测卷 英语（一） 如上文所述，我只要做真题的选择题就可以了，平时可以带着用app背考研词汇。 冲刺阶段再补一补小作文的格式和大作文的写法。\n参考书目：英语一真题集 计算机专业基础（自命题） 935是东南大学的自命题科目，包括操作系统、计算机组成原理、数据结构三个部分。 准备自命题科目的难点是资料很少，并且相比于统考科目，老师出题也比较随心。 根据往年上岸的学长学姐的经验帖，复习935可以看408的相关资料，课也可以直接跟王道的408课程。 因此我复习用的资料主要是王道考研的系列书籍。\n东南大学给出了下面四本书作为参考书目：\n《操作系统概念(第7版)》(翻译版) 西尔伯查茨(Abraham Silberschatz)、郑扣根等，高等教育出版社 2010-01 《数据结构(C语言版)》 作者：严蔚敏、吴伟民 出版社：清华大学 时间：2011-07-01 《数据结构(用面向对象方法与C++语言描述，第2版)》作者：殷人昆 出版社：清华大学 时间：2014-9-23 《计算机组成原理（第2版）》，任国林，ISBN 978-7-121-33462-7，2018.1 前三本我觉得用处不大，而计算机组成原理这部分据说是由任国林老师亲自命题，所以他写的教材是很有必要看的。\n到了强化和冲刺阶段我做了408的真题，至于935的真题我没找到资源就没做。\n参考书目：王道考研系列 操作系统/数据结构、计算机组成原理（任国林 著）、408真题、935真题（如果能找到的话） 视频课： 王道计算机考研 计算机组成原理 、 王道计算机考研 操作系统 、 王道计算机考研 数据结构 思想政治理论 政治这门课我跟的是徐涛的课，在基础阶段一边看课一边刷选择题。\n到了冲刺阶段，等肖八出来就写肖八的选择题，肖四出来后狂背客观题，今年肖秀荣老师几乎全押中了（考前还不信，没想到肖老师真的这么厉害）。\n参考书目：考研政治核心考案（徐涛）、肖1000、肖八、肖四 相关资料 [1] 东大研招网. 东南大学—蒙纳士大学苏州联合研究生院2024年硕士研究生招生简章. https://yzb.seu.edu.cn/2023/0921/c6679a466344/page.htm ⤶ [2] 东大研招网. 2023年东南大学各院系所复试分数线. https://yzb.seu.edu.cn/2023/0726/c6674a455893/page.htm ⤶ [3] 东大研招网. 2023年硕士研究生考试录取情况汇总. https://yzb.seu.edu.cn/2023/0726/c6675a455894/page.htm ⤶ [4] 东南大学计算机科学与工程学院. 2024年计算机科学与工程学院硕士研究生入学考试930、935考试大纲. https://cse.seu.edu.cn/2023/0913/c24628a464355/page.psp ⤶ ","date":"2023-12-25T11:02:45+08:00","image":"https://files.furffisite.link/blogimg/20231226003721-76bafcf83bb54ee4b8d8524cc179b790-e34bc.jpg","permalink":"https://blog.furffisite.link/p/kaoyan-stage1/","title":"考研经历 - 准备初试"},{"content":"深度Q学习算法 理论 深度Q学习算法（Deep Q-Learning Algorithm）是将Q表格替换为神经网络的Q学习算法，由 DeepMind 的 Mnih et al. [1] [2] 提出。 Q表格本质上是一个函数 $f: S\\times A \\rightarrow \\mathbb{R}$，我们自然也可以使用神经网络构造这个函数，让它可以处理连续的状态和动作。此外，使用神经网络还有一个好处：我们可以向神经网络输入实例信息 $m\\in M$ ，使之可以跨实例学习函数 $f: M\\times S\\times A\\rightarrow \\mathbb{R}$ 。也就是说，Agent可以将其在实例 $m_1,~m_2,~\\ldots~m_n$上 学习到的经验迁移到未曾见过的实例 $m_{n+1}$ 上，增强模型的泛化性能，减少其探索新实例所需的时间。\n网络更新方程的设计 （以 Bellman 方程 为基础）： $$Q(s_t, a_t) \\leftarrow (1-\\eta) Q(s_t, a_t) + \\eta(\\gamma\\max_{j\\in A} Q(s_{t+1}, j) + r_t)$$\n求更新前与更新后的差分： $$\\Delta Q(s_t, a_t) = -\\eta Q(s_t, a_t) + \\eta(\\gamma\\max_{j\\in A} Q(s_{t+1},j) + r_t)$$\n即： $$\\Delta Q(s_t, a_t) = \\eta(\\gamma\\max_{j\\in A} Q(s_{t+1},j) + r_t - Q(s_t, a_t))$$\n在理想情况下充分训练时，应当有 $$\\lim_{t\\rightarrow \\infty}\\Delta Q(s_t, a_t) = 0$$\n也就是说，训练的目标应当是最小化 $\\Delta Q(s_t, a_t)$，即目标函数为： $$L(\\theta) = \\mathrm{MSE}(Q(s_t, a_t),~\\gamma\\max_{j\\in A} Q(s_{t+1},j) + r_t)$$ 其中的 $\\mathrm{MSE}$ 也可以替换为其它的损失函数。\n实现 下面以CartPole-v1环境为例编写训练程序。\n引入相关的库以及定义一些超参数：\n1 2 3 4 5 6 7 8 9 10 11 12 from random import random, randint import gymnasium as gym import torch import torch.nn as nn from tqdm import tqdm n_actions = 2 n_states = 4 lr = 3e-4 discount = 0.95 batch_size = 128 epochs = 5000 定义神经网络，这里定义了一个简单的三层神经网络，其中输出层没有添加激活函数是因为激活函数会限制网络的值域至 $R_{act}$ ，设Q函数的值域是 $R_Q$，$R_Q\\nsubseteq R_{act}$ 时损失函数难以收敛，影响训练效果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Net(nn.Sequential): norm_vector = torch.tensor([0.5, 1.0, 0.21, 0.5]) def __init__(self, in_feats = n_states, out_feats = n_actions, hidden = 32): super().__init__( nn.Linear(in_feats, hidden), nn.SiLU(), nn.Linear(hidden, hidden), nn.SiLU(), nn.Linear(hidden, out_feats), ) def forward(self, state): x = state/self.norm_vector # 归一化 y = super().forward(x) return y 定义网络、优化器与损失函数：\n1 2 3 net = Net() optimizer = torch.optim.AdamW(net.parameters(), lr=lr, amsgrad=True) criterion = nn.SmoothL1Loss() # 发现L1的效果比L2要好 训练过程（原环境提供的reward恒为1，信息太少，因此这里改用自定义的reward，在倾角过大或位置过远时进行惩罚）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 env = gym.make(\u0026#34;CartPole-v1\u0026#34;) state, info = env.reset() for t in tqdm(range(epochs)): # 前向传播 loss = 0.0 epsilon = 1 - t / epochs # 动态调整epsilon for _ in range(batch_size): # 选择action ========================= row = net(state) if random() \u0026lt; epsilon: # exploration action = randint(0, n_actions-1) else: action = row.squeeze().argmax().item() # 执行action ========================= state, reward, terminated, truncated, info = env.step(action) # 使用自定义的reward reward = -20.0 if terminated else 0 if abs(state[2]) \u0026gt; 0.1: # 限制倾角 reward += -1.0 if abs(state[0]) \u0026gt; 0.3: # 限制位置 reward += -2.0 if reward \u0026gt;= 0: reward = 1.0 # 计算loss ========================= with torch.no_grad(): if terminated: curr_q = torch.tensor(reward) else: curr_q = net(state).max() * discount + reward loss += criterion(row[action], curr_q) if terminated or truncated: state, info = env.reset() # 反向传播 optimizer.zero_grad() (loss/batch_size).backward() torch.nn.utils.clip_grad_value_(net.parameters(), 1) optimizer.step() env.close() # 保存checkpoint torch.save(net.state_dict(), \u0026#34;cartpole.ckpt\u0026#34;) 推理：\n1 2 3 4 5 6 7 8 9 10 11 env = gym.make(\u0026#34;CartPole-v1\u0026#34;, render_mode=\u0026#34;human\u0026#34;) state, info = env.reset() with torch.no_grad(): for t in range(2000): row = net(state) action = row.squeeze().argmax(0).item() print(row) state, reward, terminated, truncated, info = env.step(action) if terminated or truncated: state, info = env.reset() env.close() 不出意外的话，运行程序后可以看到类似于这样的动画，说明这个算法以及我们编写的程序都是有效的： 完整的程序见Github Gist，模型权重可以从这里下载（虽然自己训练一个也不费事）。\n经验回放 理论 上面的训练Q网络的方式存在一些问题，例如\n样本的利用率低：每次采样只对应一次前向传播，采样得到的样本未被充分利用； 样本的时序关联性大：每次采样在时间上是高度相关的，上一次采样的末状态就是下一次采样的初始状态，影响训练效果； 训练速度慢：每次前向传播只传播一组数据，速度较慢。 为了缓解上述问题，Mnih et al. 在提出深度Q学习的同时也提出了经验回放（Experience Replay）策略 [2] 。 其主要思想是将采样与训练分离，采样时在记忆中保存采样的记录，训练时随机从记忆中选取样本进行前向与反向传播，从而降低样本间的时序关联性与提高样本利用率。\n注意到在算法中，每一步训练需要四个值：当前状态 $s_t$、动作 $a_t$、回报 $r_t$ 以及采取动作后的状态 $s_{t+1}$，因此每一次采样后只需要在记忆中保存这四个值，称为experience四元组 $e_t=(s_t,~a_t,~r_t,~s_{t+1})$。\n实现 库、超参数、网络结构以及推理部分均沿用上面的代码以便比较，只替换训练部分，然后新增Experience类与Memory类用于存储和管理样本。 以下是Experience类与Memory类的代码，这里使用队列存储最新的batch_size*10条记录：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from typing import NamedTuple, Union class Experience(NamedTuple): \u0026#39;\u0026#39;\u0026#39;experience四元组\u0026#39;\u0026#39;\u0026#39; state: np.ndarray action: int reward: float next_state: np.ndarray class Memory(object): \u0026#39;\u0026#39;\u0026#39;存储固定数量记录的队列\u0026#39;\u0026#39;\u0026#39; def __init__(self, buffer_size: int): self.buffer_size = buffer_size self.buffer: list[Union[Experience, None]] = [None for _ in range(self.buffer_size)] self.count = 0 def append(self, exp: Experience): \u0026#39;\u0026#39;\u0026#39;增加记录，如果buffer已满则替换最早的记录\u0026#39;\u0026#39;\u0026#39; self.buffer[self.count%self.buffer_size] = exp self.count += 1 def sample(self, k: int): \u0026#39;\u0026#39;\u0026#39;随机选取k个experience，打包好返回\u0026#39;\u0026#39;\u0026#39; if self.count \u0026lt; self.buffer_size: pool = self.buffer[:self.count] else: pool = self.buffer exp: list[Experience] = random.choices(pool, k=k) # type: ignore # 打包成 Tensor states = torch.from_numpy(np.array([e.state for e in exp])) actions = torch.tensor([e.action for e in exp]) rewards = torch.tensor([e.reward for e in exp]) next_states = torch.from_numpy(np.array([e.next_state for e in exp])) return states, actions, rewards, next_states memory = Memory(batch_size*10) 采样的部分与原来相同，而在训练的部分，因为这里训练时前向和反向传播都只有一步，所以在计算target_q时不需要像原文所述冻结权重，只要在其后增加.detach()确保反向传播时target_q的梯度不被传播就行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 env = gym.make(\u0026#34;CartPole-v1\u0026#34;) state, info = env.reset() batch_index = torch.arange(batch_size) for t in tqdm(range(epochs)): # 采样 ========================================= epsilon = 1 - t / epochs # 动态调整epsilon for _ in range(batch_size//4 if t \u0026gt;= 1 else batch_size): if random.random() \u0026lt; epsilon: # exploration action = random.randint(0, n_actions-1) else: action = net(state).squeeze().argmax().item() org_state = state state, reward, terminated, truncated, info = env.step(action) # 使用自定义的reward reward = -20.0 if terminated else 0 if abs(state[2]) \u0026gt; 0.1: # 限制倾角 reward += -1.0 if abs(state[0]) \u0026gt; 0.3: # 限制位置 reward += -2.0 if reward \u0026gt;= 0: reward = 1.0 # 加入记忆 memory.append(Experience(org_state, action, reward, state)) if terminated or truncated: state, info = env.reset() # 训练 ========================================= states, actions, rewards, next_states = memory.sample(batch_size) # 前向传播 pred_q = net(states)[batch_index, actions] target_q = (net(next_states).max(dim=-1).values * discount + rewards).detach() loss = criterion(pred_q, target_q) # 反向传播 optimizer.zero_grad() loss.backward() torch.nn.utils.clip_grad_value_(net.parameters(), 1) optimizer.step() env.close() # 保存checkpoint torch.save(net.state_dict(), \u0026#34;cartpole-replay.ckpt\u0026#34;) 完整程序见Github Gist，运行程序，发现两个程序在batch_size=128和epochs=5000的情况下，原来的程序在我的轻薄本上需要训练3分钟，而得益于批处理的训练过程以及采样数的减少，有经验回放的训练只要15秒就能达到更好的效果。\n继续增加batch_size或epochs，效果更佳。以下是batch_size=256、epochs=5000的结果，训练只花了28秒。\n参考文献 [1] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., \u0026amp; Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning (arXiv:1312.5602). arXiv. https://doi.org/10.48550/arXiv.1312.5602 ⤶ [2] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., \u0026amp; Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), Article 7540. https://doi.org/10.1038/nature14236 ⤶ ⤶ ","date":"2023-08-22T10:51:10+08:00","image":"https://files.furffisite.link/blogimg/20230822110113-5ef18ab6cfb63d5a730bbd8e077231e7-6ba69.jpg","permalink":"https://blog.furffisite.link/p/deep-q-learning/","title":"【RL学习笔记】深度Q学习算法与经验回放"},{"content":"我开始折腾NAS的原因很简单——笔记本的存储空间不够了，当时固态硬盘的价格远没有现在这么便宜，移动硬盘随身携带有点麻烦，所以就想给硬盘连上网，这样只要有网就能访问。\n当时是2021年3月，我手边有闲置的树莓派Raspberry Pi 3B+（19年花￥279买的），于是一个很朴素的想法就形成了——买一个SATA-USB的硬盘盒连接机械硬盘，再给树莓派连上网线，做成一个简陋的NAS（后来才知道有NAS这个名字）。我后来也的确是这么操作的，硬盘盒买的绿联的，花了￥99。使用后发现树莓派3B+的USB2.0接口太慢了，于是半年后（2021.9）加钱买了带USB3的树莓派4B，花了￥578（当时这个价格已经很贵了，想不通为什么自己那个时候决定要买），所以这一套不算硬盘的价格是￥677。\n我使用的操作系统是树莓派官方提供的Raspbian，在上面安装了\nsamba（文件服务）、 自己写的录播姬、 aria2（BT下载）、 Coredns（因为学校的DNS服务器挂过一次）。 我认为树莓派作为NAS主要的优点是：\n结构简单，插电就能用，且易于携带； 耗电量很低； 自带GPIO接口，可以外接一些LED或LCD 1602显示服务器状态等，或是增加传感器以收集数据，这对于“折腾类”玩家又是一个可折腾的点。 缺点是：\n第一，它的cpu是32位arm架构的，这导致许多没有提供32位arm版本的程序它都无法运行，例如qbittorrent、数据库、jellyfin等，而受限于其性能，从源代码编译这条路也不简单（当然也可以使用docker buildx跨平台编译，但是当时的我还不会搞这些）； 第二，性能太弱； 第三，性价比不高。 相较于其优点，它的缺点还是太突出了，于是我在一年后（2022.10）换了amd64架构的NAS。\n","date":"2023-08-21T14:46:05+08:00","image":"https://files.furffisite.link/blogimg/20230821143533-8f90300e7a87f5103e8c13359c0ce9a7-a82cd.jpg","permalink":"https://blog.furffisite.link/p/nas-1/","title":"我折腾NAS的历程（一）"},{"content":"时值2022年国庆节，我在淘宝翻了很久，找x86/amd64的NAS方案，最终相中了淘宝某家店卖的“青春版黑群晖”，CPU+主板+风扇+4GB 内存+亚克力外壳一共只要￥285，外带黑群晖的引导盘。这个价格很吸引我，于是我没过多考虑就下单了。\n货到之后发现这个主板似乎是工控机里拆出来的，CPU是12年Q1出的Atom D2550。主板上只有一个SATA口，其余的被拆掉了，于是商家增加了一个SATA转M.2接口的板子插在（疑似）M.2口上，这样就能支持双硬盘了。因为我想让硬盘完全用来存储数据，所以我又花￥56.9买了一个64GB的小U盘用来存储操作系统。\n操作系统选的是Ubuntu Server 16.04，理由是相较于商家提供的黑群晖，我更熟悉Ubuntu。软件和服务方面安装了samba、jellyfin、qbittorrent、coredns等。\n这个价格我很是满意，但是这台NAS的性能不比树莓派高多少，毕竟是十年前的处理器，导致它在一些高负载的情况下（例如Jellyfin使用ffmpeg转码播放视频）有些力不从心，播放时异常卡顿，所以在今年六月的时候趁购物节自己组了一台。换了新的之后发现，性能上去了，能同时跑的服务就多了很多，可以真的把它当作一台服务器来用了。\n","date":"2023-08-20T10:22:44+08:00","image":"https://files.furffisite.link/blogimg/20230821152623-0e0e9fdf80c6f1c9ffeee0f80f808e1e-60da9.jpg","permalink":"https://blog.furffisite.link/p/nas-2/","title":"我折腾NAS的历程（二）"},{"content":"组这台NAS前的事情：\n我折腾NAS的历程（一） 我折腾NAS的历程（二） 理论 装机配置单 品牌 型号 参数 购买价 购买链接 CPU Intel E5-2660 v4 14核 2.0GHz 规格表 ¥52（洋垃圾） link 内存 Samsung M393A2G40DB0-CPB DDR4 2133MHz 16GB 规格表 ¥92（洋垃圾） link 主板 华南金牌 X99-4MF ITX DDR4x4 SATAx3 规格表 ¥268 link 系统盘 SKHynix HFS512GDE9X084N 512GB NVME ¥0* 显卡 NVIDIA GeForce 605 规格表 ¥16（亮机卡） link 电源 航嘉 GX500 500W ATX 白牌 ¥191.92 link 机箱 invasion invasion X1 4盘位ITX ¥244 link CPU散热器 零下30度 - - ¥38.7 link 机箱风扇 航嘉 MVP120 1500RPM，支持PWM ¥75.95（三只装） link 总计 ¥978.57 *笔记本上拆下来的\n攒机理念 目标：省钱、性能还行、体积小、静音、保留一定的可升级性、尽量一手。\n关于CPU，我在CPUbenchmark上按照跑分结果在E5的一系列洋垃圾型号中从高到低翻，最后看中了 E5-2660 v4。选择它有以下几点理由：\n其功耗相比与同系列其它型号较低； E5-2660 v4为2016 Q4推出的，相对v3和v2要新一点； 其性价比在同型号中较高，52元16005分，相较于2690 v3的68元16505分更实惠。 CPU定下来了之后，主板和内存也就相应的决定了。之所以主板不选二手是因为我想省点事，对于二手市场上可能遇到的众多问题我目前没有任何应对经验。而选择ITX是为了满足体积小的要求，毕竟宿舍里放不下太多东西。\n关于系统盘，之前给笔记本升级外存的时候拆下来的原装SSD可以直接给它用。就算是买一个全新的也不会太贵，256GB买个二线厂的SSD应该也够用了。 在装系统、进BIOS时显卡是必要的，作为服务器CPU的E5-2660 v4没有核心显卡，因此这里依然需要一张显卡。上淘宝一搜果然有卖亮机卡的，16元的价格也就不用纠结值不值了，能亮就行。为了能看到视频输出我还买了一个二十多的HDMI采集卡以代替显示器，事实证明二十块的画质果然不行。 电源不敢买二手的，怕出问题。500W的电源相对于目前的整机功耗明显是过剩的，但这也为以后的升级保留了余量。 最后是机箱，综合以上要求，机箱应该是体积小、轻便、便宜的多盘位ITX机箱，在淘宝上翻了一大圈只看到这一款同时满足这几点的。别的型号，例如蜗牛星际、御夫座、天箭座等都太贵了或者散热容量太小。两百多的机箱我觉得还是贵了点，可是也没找到更好的选择。\n关于可升级性，这套配置中，CPU可以升级为24686分的E5-2699 v4（在价格降下来之后），还有三个位置可以加内存条，显卡可以升级，机箱和电源以后也能用于装游戏电脑。\n实践 装机过程 装机过程中遇到的问题有四：\nCPU散热器的螺丝是弯的（可能是制造问题），要用钳子扳直了才能安装； 机箱自带的硬盘笼和主板电源插槽有2mm左右的overlap，装完之后硬盘笼有一点点变形。得益于硬盘架的减震机构，这个变形产生的影响不大； 主板下部的插槽和机箱风扇有冲突，前面板USB-3的线插不进去，但对于NAS而言这根线不插问题也不大； 华南金牌的这款主板的主板风扇插槽只有3针，不支持PWM，直接插MVP120的话满转速噪音非常大。我最后用一拖三的线把所有风扇都接到了支持PWM的CPU风扇插槽上，这样就安静了很多（之后又进BIOS调整了一下温度-转速曲线）。 操作系统 因为我最熟悉的Linux发行版是Ubuntu，所以我安装了Ubuntu 22.04.2 LTS（服务器版）。\n系统盘分区如下：\n1 2 3 4 5 6 Mountpoint Start End Sectors Size Type /boot/efi 2048 2203647 2201600 1G EFI System / 2203648 107061247 104857600 50G Linux filesystem /home 107061248 211918847 104857600 50G Linux filesystem /srv 211918848 958271487 746352640 355.9G Linux filesystem swap 958271488 1000214527 41943040 20G Linux swap 测试 整机待机时CPU核心平均温度为30°C，满载时为55°C，说明散热还是有很大的余量的。服务正常运行时插座上测量到的功率在68W左右，CPU满载时约140W。\n使用sysbench的测试结果如下，和我现役的游戏本相比我觉得这个成绩还算可以接受：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 sysbench 1.0.20 (using system LuaJIT 2.1.0-beta3) Running the test with following options: Number of threads: 28 Initializing random number generator from current time Prime numbers limit: 10000 Initializing worker threads... Threads started! CPU speed: events per second: 17251.86 General statistics: total time: 600.0017s total number of events: 10351174 Latency (ms): min: 1.22 avg: 1.62 max: 82.58 95th percentile: 1.64 sum: 16797418.36 Threads fairness: events (avg/stddev): 369684.7857/436.76 execution time (avg/stddev): 599.9078/0.00 软件部分 Docker 最先安装的是Docker，因为有了Docker之后别的应用都可以使用Docker镜像安装和管理。\n在Ubuntu上安装Docker只需使用官方提供的安装脚本 [1] ：\n1 2 $ curl -fsSL https://get.docker.com -o get-docker.sh $ sudo sh get-docker.sh 修改Docker数据位置 Docker的数据默认储存在/var/lib/docker内，我想将其移到/srv文件系统内，操作如下：\n关闭docker服务：systemctl stop docker 移动文件：sudo mv /var/lib/docker /srv/ 创建符号链接（以防万一）：sudo ln -s /srv/docker /var/lib/docker 修改配置文件，在/etc/docker/daemon.json内添加配置：{\u0026quot;data-root\u0026quot;: \u0026quot;/srv/docker\u0026quot;} 启动docker服务：systemctl start docker 使用Docker国内镜像源 阿里云提供了免费的Docker镜像源的加速服务。为了使用该服务，需要在 https://aliyun.com 登录阿里云帐号，然后在 控制台 \u0026gt; 容器镜像服务 \u0026gt; 镜像工具 \u0026gt; 镜像加速器 处可以获取加速器的地址及修改镜像源的方法。\n此外也有一些开放的镜像源，在/etc/docker/daemon.json内添加如下属性 [2] ：\n1 2 3 4 5 6 7 8 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://ustc-edu-cn.mirror.aliyuncs.com\u0026#34;, \u0026#34;https://ghcr.io\u0026#34;, \u0026#34;https://mirror.baidubce.com\u0026#34; ] } 修改后使用 systemctl restart docker 重启docker服务即可。\n手动下载镜像 如果替换了镜像源之后也拉不动镜像的话，可以试试 Moby Project 提供的镜像下载脚本下载镜像：link\n使用例：\n1 2 3 4 5 6 7 8 9 $ wget https://raw.githubusercontent.com/moby/moby/master/contrib/download-frozen-image-v2.sh $ bash ./download-frozen-image-v2.sh ./alpine-linux alpine:latest Downloading \u0026#39;library/alpine:latest@latest\u0026#39; (1 layers)... -#O=-# # # ############################################## 100.0% Download of images into \u0026#39;./alpine-linux\u0026#39; complete. Use something like the following to load the result into a Docker daemon: tar -cC \u0026#39;./alpine-linux\u0026#39; . | docker load 这时镜像就已经保存到./alpine-linux内了，然后使用\n1 2 3 $ tar -cC \u0026#39;./alpine-linux\u0026#39; . | docker load ...... Loaded image: alpine:latest 即可让docker导入镜像。\nsamba 1 $ sudo apt install samba 安装后使用smbpasswd -a \u0026lt;username\u0026gt;为用户添加samba的访问密码，其中\u0026lt;username\u0026gt;必须为已经在Linux内注册的用户。\n配置文件为/etc/samba/smb.conf，配置项可参考官方文档。\n1 2 3 4 5 6 [\u0026lt;share name\u0026gt;] path = /path/to/folder read only = no browsable = yes guest ok = no valid users = \u0026lt;users\u0026gt; jellyfin jellyfin是数字媒体管理与串流软件，它支持浏览器、Android、iOS、Android TV等许多客户端，可以在这些客户端上便捷地观看服务端存储的内容，这也是我攒这台NAS的主要目的。E5-2660 v4的性能足以支持jellyfin流畅编解码超高清资源（软解，不过目前看来也没有专门为这个买GPU的必要了）。\n可以使用官方的docker镜像或者linuxserver.io制作的镜像来安装jellyfin。\n为了便于我管理和配置，所有的Docker容器都将使用docker compose启动。这是jellyfin的docker-compose.yaml：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 version: \u0026#34;2.1\u0026#34; services: jellyfin: image: lscr.io/linuxserver/jellyfin:latest container_name: jellyfin environment: - PUID=1000 - PGID=1000 - TZ=Asia/Shanghai volumes: - ./config:/config - /path/to/media:/data ports: - 8096:8096 - 8920:8920 #optional - 7359:7359/udp #optional - 1900:1900/udp #optional restart: unless-stopped deploy: mode: global resources: limits: cpus: \u0026#39;20\u0026#39; memory: 8G qbittorrent qbittorrent是用于bt下载的软件。linuxserver.io也打包了qbittorrent的镜像，使用这个镜像运行即可。我的docker-compose.yaml如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 services: qbittorrent: image: lscr.io/linuxserver/qbittorrent:latest container_name: qbittorrent environment: - PUID=1000 - PGID=1000 - TZ=Asia/Shanghai - WEBUI_PORT=4823 volumes: - ./config:/config - ./torrents:/torrents - ./cache:/cache - /path/to/downloads:/downloads ports: - 4823:4823 # WebUI - 6881:6881 # 监听端口 - 6881:6881/udp - 14560-14580:14560-14580 # 传出端口 - 14560-14580:14560-14580/udp restart: unless-stopped deploy: mode: global resources: limits: cpus: \u0026#39;2.0\u0026#39; memory: 1G frpc 内网穿透工具的客户端，我用的是下载量最高的镜像。\ncoredns DNS服务器，因为学校的DNS以前崩过，所以装一个备用。官方镜像即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 services: coredns: image: coredns/coredns:latest container_name: coredns volumes: - ./config:/config command: [\u0026#34;-conf\u0026#34;, \u0026#34;/config/Corefile\u0026#34;] restart: always ports: - 0.0.0.0:53:53 - 0.0.0.0:53:53/udp deploy: mode: global resources: limits: cpus: \u0026#39;0.5\u0026#39; memory: 100M 53端口可能和ubuntu的systemd-resolved服务冲突，在启动容器前需要先关闭这个服务：\n1 2 $ sudo systemctl disable systemd-resolved $ sudo systemctl stop systemd-resolved coredns结合nginx/Apache转发与一些路由器的设置就可以实现内网域名（不用记端口号了），例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ### Corefile: ...... server.local { hosts { 192.168.0.xx jellyfin.server.local } log errors prometheus } ### nginx config: server { listen 80; server_name jellyfin.server.local; location / { proxy_pass http://127.0.0.1:8096; } } 这样在局域网直接访问 http://jellyfin.server.local 就可以进入jellyfin的界面了。\n注：关闭systemd-resolved服务之后在使用KVM时可能会遇到一些问题。\nGrafana \u0026amp; Prometheus Grafana和Prometheus这两个开源软件组合在一起可以用来监控服务器状态（不过在自用的NAS上搞这个可能意义不大）。之前没了解过这些软件，因此这部分完全是按照教程安装的，其中Grafana不涉及数据收集，可以在Docker容器中运行。\n除了这两个软件，我还安装了node exporter和cadvisor，分别收集服务器系统与Docker容器的统计信息。\n装好之后，我以node exporter为基础改出了这样的dashboard： 数据库 有一些服务（例如gitea和自己写的爬虫）的数据是存储在数据库中的，我比较喜欢将这些数据放在同一个数据库服务端内（而非给每个有需要的应用都运行一个数据库），以便于数据的管理，同时减少系统占用。\n关于关系型数据库，我使用的是mariadb（MySQL的一个衍生版本）。关于镜像的版本，建议使用一个固定的版本号（而非latest），因为在不同版本的mariadb之间迁移数据还是比较麻烦的。\n关于数据库的管理端，我以前常用的是phpmyadmin，这次想试试别的，所以使用了官方docker镜像页里提到的adminer。其功能虽不如phpmyadmin丰富，界面也不如它美观，但是我平时会用到的基础操作（建表、查看数据、SQL查询、导出数据）它都有，缺少的功能可以直接用SQL语句凑嘛~\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 services: db: image: mariadb:10.11.4 container_name: mariadb restart: always environment: MARIADB_ROOT_PASSWORD: ... volumes: - ./mariadb:/var/lib/mysql ports: - 3306:3306 adminer: image: adminer container_name: adminer restart: always ports: - 8080:8080 如果服务需要，也可以使用docker运行非关系型数据库MongoDB和redis。\nMinecraft服务器 （papermc） 选了docker hub里搜索排名靠前的 marctv/minecraft-papermc-server。\n用了之后才发现，papermc是SpigotMC的分支，旨在提供原生Minecraft的体验。也就是说它不支持以前我在玩Minecraft时耳熟能详的那些Forge版mod，只支持服务器端的bukkit插件（plugin）。好处是相比与Forge服务器占用更小，并且不挑客户端，只要客户端Minecraft版本和服务器一致就能连上。 配置好之后，使用frpc转发25565端口到公网，就可以愉快地和朋友们一起玩啦。\n后台管理 在docker-compose.yaml中对应的service配置里加上\n1 2 3 stdin_open: true tty: true container_name: mcserver 然后\n1 $ docker compose up -d 之后便可以通过\n1 $ docker attach mcserver 进入服务器后台，使用指令管理服务器。\n配置 papermc相比于原版服务端修复了活塞相关的一些“特性”，例如使用活塞复制物品等，并在配置文件中默认限制这些行为，导致刷地毯机/破基岩等原版可行的操作在papermc服务端内不可用。当然这样对于服主而言确实可以防止玩家做出越界的行为，但是对于我这种只有几个朋友在一起玩的服务器，就不需要限制这些了。关闭限制的方式很简单，修改数据目录下的paper.yml，将allow-headless-pistons、allow-permanent-block-break-exploits和allow-piston-duplication三项设置为true即可。\n安装插件 Papermc支持的插件可以在 Hanger、bukkit或SpigotMC下载。下载对应版本的jar文件放到 /data/plugins 文件夹下（或者直接使用 wget/curl 下载），然后重启服务器，在启动时如果新增的插件没有报错误信息，那大概率是没问题的。\n目前加了这些插件：\nMinepacks：背包插件； TreeFeller：连锁砍树； worldedit：创世神插件； worldguard EssentialsX：正如其名，提供一些服务器的必备功能； StackableItems：增加物品堆叠上限； Multiverse-Core：多世界基础插件。 Multiverse-signportals：使用告示牌在多世界间传送； OpenTerrainGenerator：创建不同类型世界的基础插件之一。 Skylands FarFromHome ViaVersion：放宽客户端版本限制； LaggRemover：减少服务器卡顿； BlueMap: 地图插件，提供了美观的在线3D地图； UnifiedMetrics：数据收集插件，支持前述的Prometheus。 参考资料 [1] Install Docker Engine on Ubuntu - Docker Docs | https://docs.docker.com/engine/install/ubuntu/#install-using-the-convenience-script ⤶ [2] Docker 换源 - 腾讯云开发者社区 | https://cloud.tencent.com/developer/article/1769231 ⤶ ","date":"2023-06-18T13:58:40+08:00","image":"https://files.furffisite.link/blogimg/20230618181015-7e414fb06bb70924b269f0ca2a5ba8f6-28296.jpg","permalink":"https://blog.furffisite.link/p/nas-3/","title":"我折腾NAS的历程（三）"},{"content":"算法 Q学习算法（Q-Learning Algorithm）的思路比较简单：使用Q函数记录每一个状态下每一个动作（action）的期望最大总回报（reward），即Q值。在推理时贪心地选择当前状态Q值最大的动作，从而达到最大化期望总回报的目的。当问题的状态与动作均为离散时，Q函数可以使用表格记录，这个表格也称为Q表格（Q-Table）。\n设：\n问题的状态空间为 $S = {1,~2,~\\ldots, m}$； 问题的动作空间为 $A = {1,~2,~\\ldots, n}$； 探索阈值为 $\\epsilon\\in [0,1]$（推理时 $\\epsilon = 0$）； 学习率 $\\eta\\in [0,1]$； 回报衰减率 $\\gamma\\in [0,1]$。 则Q学习算法的流程如下：\n初始化Q表格为零矩阵 $Q=O_{m\\times n}$； 设初始时间 $t = 0$，状态为 $s_t = s_0$； 选择动作 $a_t$：取随机数 $r\\in[0,1]$，若 $r\u0026lt;\\epsilon$，则当前为探索阶段，从 $A$ 随机选取一个动作；否则 $a_t = \\argmax_{j\\in A} Q_{s_t, j}$； 与环境交互，执行动作 $a_t$，并获得状态 $s_{t+1}$、回报 $r_t$； 按照 Bellman 方程 更新Q表格： $$Q_{s_t, a_t} \\leftarrow (1-\\eta) Q_{s_t, a_t} + \\eta(\\gamma\\max_{j\\in A} Q_{s_{t+1},j} + r_t),$$ 其中 $\\max Q_{s_{t+1},j}$ 为 转移后的状态 $s_{t+1}$ 下最大的Q值，加上 $r_t$，组成转移前状态 $s_t$ 下 $a_t$ 操作的Q值。将其与原来的 $Q_{s_t, a_t}$ 加权平均就得到了更新后的Q值。 时间 $t\\leftarrow t+1$，回到第 3 步。 实现 Frozen Lake 以 Gym 的 Frozen Lake 环境为例，其状态空间与动作空间都是离散的，因此适合使用Q表格。这个环境共有 $4^2=16$ 种状态，4种动作，分别对应上下左右四个移动方向。环境中slippery引入的不确定性太大（每次行动只有$1/3$的概率能真正前往指定的方向），因此这里创建环境时is_slippery一项设为False。\n实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import gymnasium as gym import numpy as np from random import random env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, map_name=\u0026#34;4x4\u0026#34;, is_slippery = False) n_actions, n_states = env.action_space.n, env.observation_space.n lr, gamma = 0.2, 0.6 total = 1000 Q_table = np.zeros((n_states, n_actions)) state, info = env.reset() for t in range(total): # 选择动作 epsilon = 1 - t / total if random() \u0026lt; epsilon: # 探索 action = env.action_space.sample() else: action = Q_table[state].argmax(0) org_state = state state, reward, terminated, truncated, info = env.step(action) if state == n_states-1: # 抵达终点，获得奖励 reward = 20 elif terminated: # 掉进洞里，给予惩罚 reward = -5 else: # 让agent尽快抵达终点 reward = -1 # 更新Q值 Q_table[org_state, action] = ((1 - lr) * Q_table[org_state, action] + lr * (reward + gamma * Q_table[state].max())) if terminated or truncated: state, info = env.reset() 原环境提供的回报函数所含的信息较少（仅在抵达终点时为1，其余情况均为0），不利于算法的收敛，因此我在实现中重新设计了回报函数。 此外，训练时采用线性衰减的探索阈值 $\\epsilon$ ，即训练初期倾向于探索（exploration），后期倾向于开发（exploitation）。\n训练时每轮的t与该轮获得的总reward的折线图如下：\n推理时算法选择的路线如下图，确实是最优路线之一。\n除了 Frozen Lake，这个方法也可以用来求解 Gym 提供的 Cliff Walking、Taxi 与 Blackjack。但是，随着问题规模的增大，训练步数也需要相应地增加。\nCliff Walking 这个环境有48种状态和4种动作，因此Q表格内共有192项。以下是训练了 10,000 步的结果（平均每项52步）：\nTaxi 这个环境有500种状态和6种动作，对应Q表格的3000项。以下是训练了 200,000 步的结果（平均每项67步）：\nBlackjack Blackjack 有 $32\\times 11\\times 2=704$ 种状态和 $2$ 种动作，在处理时需要将离散的状态向量映射到非负整数域内。 这个游戏的状态转移存在不确定性，即使是充分训练（$5\\times 10^6$ 步）的Q表格也只能将胜率从完全随机时的 28.2% 提升到 39.3%。\n分析 优点 算法简单，易于理解和实现。 局限性 基于Q表格的Q学习算法只能处理输入输出都是离散的问题； 基于Q表格的Q学习算法不能跨实例学习，即在遇到新的问题实例时，需要从0开始重新探索； 基于Q表格的Q学习算法难以处理训练过程中没有见过的状态； 当状态空间或动作空间很大时，Q表格的规模也会很大，从而需要更长的学习时间。 上述问题可以通过引入神经网络缓解，即深度Q学习算法（Deep Q-Learning Algorithm）。\n","date":"2023-05-18T15:05:05+08:00","image":"https://files.furffisite.link/blogimg/20230518191643-b6c8849bfb006d40abef7203bb5e15a0-18b9a.jpg","permalink":"https://blog.furffisite.link/p/q-learning/","title":"【RL学习笔记】Q学习算法"},{"content":"因为Linux端的Zotero（AUR）中有一部分元素的样式是由GTK控制的，因此当系统的GTK主题为深色主题时，Zotero的界面会呈现为这个样子： 这不是很好看，并且部分区域内文字和背景色的对比度很低，导致文字难以阅读。\n根据这个帖子，解决方案是通过环境变量在程序启动时指定其使用的GTK主题，也就是：\n1 $ GTK_THEME=Pop-light zotero 反映到Desktop文件上（通常位于/usr/share/applications/和~/.local/share/applications/），就是在启动指令（Exec项）前加入env GTK_THEME=Pop-light。\n1 2 3 4 5 [Desktop Entry] Type=Application Name=Zotero Exec=env GTK_THEME=Pop-light /usr/bin/zotero -url %U ... 这样操作后，通过应用启动器打开的Zotero就不会有问题了。\n","date":"2023-05-13T13:12:27+08:00","permalink":"https://blog.furffisite.link/p/linux-darkmode-zotero-gui-issue/","title":"解决Linux系统深色模式下的Zotero显示问题"},{"content":"问题背景 在使用Python的Flask框架开发Web应用的过程中，一个基本的服务端程序结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from flask import Flask app = Flask(__name__) @app.route(\u0026#39;/handler1\u0026#39;) def handler1(): ... @app.route(\u0026#39;/handler2\u0026#39;) def handler2(): ... @app.route(\u0026#39;/handler3\u0026#39;) def handler3(): ... 可以按照这种模式无限添加处理视图（handler），但是随着项目增大，这种将所有 handler 都放在一个py文件里的模式显然是不合适的，这时可以使用 blueprint 将每个handler（或一组handler）放在互相独立的文件里。\n项目结构如下：\n1 2 3 4 5 6 7 . ├── app.py └── services ├── __init__.py ├── login.py ├── register.py ... 代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \u0026#34;\u0026#34;\u0026#34; === app.py === \u0026#34;\u0026#34;\u0026#34; from flask import Flask from services import blueprint app = Flask(__name__) app.register_blueprint(blueprint) \u0026#34;\u0026#34;\u0026#34; === services/__init__.py === \u0026#34;\u0026#34;\u0026#34; from flask import Blueprint blueprint = Blueprint(\u0026#39;api\u0026#39;, __name__) # 为了在程序启动过程中能运行两个handler文件，需要在这里import它们 from . import login from . import register \u0026#34;\u0026#34;\u0026#34; === services/login.py === \u0026#34;\u0026#34;\u0026#34; from . import blueprint @blueprint.route(\u0026#34;/login\u0026#34;) def handle_login(): # 处理登录逻辑 ... \u0026#34;\u0026#34;\u0026#34; === services/register.py === \u0026#34;\u0026#34;\u0026#34; from . import blueprint @blueprint.route(\u0026#34;/register\u0026#34;, methods=[\u0026#34;POST\u0026#34;]) def handle_register(): # 处理注册逻辑 ... 这样做存在两个问题：\n每次新增一个文件都需要在__init__.py中添加相应的 import 语句，较为麻烦； PEP-8 中要求将 import 语句放在文件的顶部，__init__.py显然不符合（但必须如此），因而静态检查器有可能在此处报错（E402）。 于是就自然而然地想到了：如何在模块初始化时自动导入当前路径下的所有子模块呢？\n解决方法 将services/__init__.py改为这样：\n1 2 3 4 5 6 7 8 import pkgutil import importlib from flask import Blueprint blueprint = Blueprint(\u0026#39;api\u0026#39;, __name__) for spec in pkgutil.iter_modules(path=__path__, prefix=\u0026#39;\u0026#39;): importlib.import_module(\u0026#34;.\u0026#34;+spec.name, __name__) 原理探究 可以通过pdb或 print 获得这段程序所涉及变量的值：\n1 2 3 4 __name__ = \u0026#39;services\u0026#39; __path__ = [\u0026#39;/tmp/test/services\u0026#39;] spec = ModuleInfo(module_finder=FileFinder(\u0026#39;/tmp/test/services\u0026#39;), name=\u0026#39;login\u0026#39;, ispkg=False) spec = ModuleInfo(module_finder=FileFinder(\u0026#39;/tmp/test/services\u0026#39;), name=\u0026#39;register\u0026#39;, ispkg=False) 其中__name__为当前包的名字，__path__为文件所在文件夹的路径。 pkgutil模块的iter_modules函数会找到提供的path下的所有子模块，在这里就是login与register，并返回它们的 ModuleInfo。\n在for循环内提取ModuleInfo的name，得到模块的名字，加上前缀.，即当前目录下的对应子模块。最后使用importlib的import_module函数导入子模块（即运行子模块的代码，将handler注册到router上）。这样无论增加多少handler文件，__init__.py都可以找到并加载它们。\n","date":"2023-05-11T12:22:54+08:00","image":"https://files.furffisite.link/blogimg/20230511163144-c5d0b19ce502665b1891ef8c78c5b839-32aa3.jpg","permalink":"https://blog.furffisite.link/p/python-import-directory-modules/","title":"在Python中导入当前路径下的所有模块"},{"content":"starship 是使用Rust编写的轻量且迅速的终端提示符程序，其功能和作用与Oh My Zsh 相似，但是相比于Oh My Zsh，starship具有以下优点：\nstarship是跨平台跨终端的，其支持Bash、Zsh、Fish等十几种终端，甚至包括Windows的PowerShell与cmd； 使用编译型语言Rust编写的starship在运行速度上优于基于shell script的Oh My Zsh； starship的自定义配置方法比Oh My Zsh简单。 先上图，以下是我所习惯的配置的效果图。除当前用户、hostname和当前工作路径外，starship还显示了git状态、相关软件的版本、进程的返回码、运行时间、已用内存/虚拟内存和当前时间等信息。starfish是通过当前工作目录下的文件名判断应当展示哪些模组的，所以当我创建文件之后提示符上也就多出了相关软件的信息。\n下文将简述starship的安装方法，并给出我的配置文件。\n安装 本文仅适用于在Linux下的bash和zsh终端安装starship，在其它终端的安装方法请参见starship的官方文档。\n首先使用官方脚本在Linux系统内安装starship程序：\n1 $ curl -sS https://starship.rs/install.sh | sh 这时如果在终端输入starship -V能看到starship的版本信息，就说明程序安装成功了。\n然后需要配置终端程序，使其能使用starship作为提示符。在~/.bashrc（bash终端）或~/.zshrc（zsh终端）内加入这一行即可：\n1 eval \u0026#34;$(starship init bash)\u0026#34; 配置 starship的配置文件是~/.config/starship.toml，你也可以通过设置环境变量STARSHIP_CONFIG改变此文件的位置。在终端输入starship config可以直接打开该文件。\n由后缀名可知，starship的配置文件为TOML文件，遵守TOML语法，关于TOML语法本文就不赘述了，若需要了解详情请移步TOML官网。\nstarship是分模块的结构，starship生成的提示符中的每一个部分都对应starship的一个模块，你可以使用starship explain指令查看各模块的说明及其运行时间，例如以我当前的配置输入该命令后效果如下：\n关于如何自定义配置，starship的官方文档已经写的很完善了，参见 https://starship.rs/zh-CN/config/。\nstarship在官方文档里已经给出了一些预设，你可以以你喜欢的预设为基础进行定制，例如我的配置就是在Bracketed Segments和Nerd Font Symbols预设之上按照自己的习惯所做的更改。我的starship.toml如下：\n另外，使用Nerd Font Symbols需要Nerd Font，请前往 https://www.nerdfonts.com/font-downloads 下载你习惯的字体对应的图标字体，并在虚拟终端中将其设为默认字体。\n相关问题 在使用Anaconda管理Python的虚拟环境时，Anaconda会自动在提示符前加上当前虚拟环境的名称，如(base)，这与starship冲突了（且starship的conda模块提供了相同的功能），因此需要使用如下指令禁用Anaconda的这个功能。\n1 $ conda config --set changeps1 False ","date":"2023-01-06T16:24:31+08:00","image":"https://files.furffisite.link/blogimg/20230109230402-9ec5accbc75f863015a4dca68f9f9870-cc3be.jpg","permalink":"https://blog.furffisite.link/p/use-starship/","title":"使用starship定制终端提示符"},{"content":" 今天在QQ看到一张图，这张图在预览下看到的和查看图片时看到的内容不一样。虽然不是第一次见这种图了，但是今天在摸鱼的时候无事可做，就研究了一下它的原理。\n这张图是这样的：左图为它在聊天界面的样子，典型的吃🍑场景，点开大图之后看到的却是右图的样子（笑）。 中间的图片为嵌入的原图，如果您开启了暗色模式（桌面端可以在页面右下角切换），您看到中图的应该和右图一样，反之则和左图一样。 原图可以在这里获取。\n原理 这种图的原理并不难猜，肯定利用了png图片的透明度通道，使其在不同背景色下能呈现出不同的效果。下面求解一下它是怎么制作出来的： （因为这里只涉及图像的像素变换，为简化表示，以下的每个变量都表示单个像素单个通道的值，且定义域为$[0,1]$.）\n设在聊天界面看到的图像（表图）灰度为$c$，点开大图后看到的图像（里图）为$h$，待求量为生成的图像的灰度$g$与透明度$a$。\n在聊天界面的图像是目标图像和白色背景的混合，也就是$$(1-a) + g\\cdot a = c.$$ 点开大图后是目标图像和黑色背景的混合，即$$0+g\\cdot a = h.$$\n解出来就是$$a=1+h-c,~g=h/a.$$\n又因为解需要满足$a,g\\in(0,1]$，所以$c$和$h$需要满足不等式$$h\\le c\u0026lt;1+h$$ 右侧当且仅当$c=1,~h=0$时不成立。\n实现 这个程序实现起来没有什么难度，python代码如下（需要Pillow与numpy）。需要注意的是，为了满足上面的不等式，在程序中需要重新分配两张原图的灰度的范围。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import numpy as np from PIL import Image # 读取图片+转灰度 cover = Image.open(\u0026#34;./cover.png\u0026#34;).convert(\u0026#34;L\u0026#34;) hidden = Image.open(\u0026#34;./hidden.png\u0026#34;).convert(\u0026#34;L\u0026#34;) assert cover.size == hidden.size cover = np.asarray(cover) hidden = np.asarray(hidden) # 按照约束条件调整像素范围 cover_min, cover_max = cover.min(), cover.max() hidden_min, hidden_max = hidden.min(), hidden.max() cd = cover_max - cover_min hd = hidden_max - hidden_min divide_portion = hd/(hd*1.0+cd) cover_f = (cover-cover_min)*1.0/cd * (1-divide_portion-1/255) + divide_portion hidden_f = (hidden-hidden_min)*1.0/hd * divide_portion # 计算 alpha = 1-cover_f+hidden_f gray = hidden_f/alpha alpha = (alpha*255).astype(np.uint8) gray = (gray*255).astype(np.uint8) result = np.stack([gray,gray,gray,alpha], axis=-1) # 保存结果 img = Image.fromarray(result) img.save(\u0026#34;./output.png\u0026#34;) 这是随手抓的两张黑白表情包做出来的效果，顺序和上面的展示一样，您可以通过切换暗色模式查看效果：\n注：在QQ发送此类图片时，一定要选择发送原图，否则QQ可能会在压缩过程中去掉透明度通道，导致其失效。\n","date":"2023-01-03T23:51:54+08:00","image":"https://files.furffisite.link/blogimg/20230104022542-b692e6e89157626a929575a0b97e5583-3e819.jpg","permalink":"https://blog.furffisite.link/p/qq-duplicity-image/","title":"利用透明度通道制作QQ内表里不一的图片"},{"content":"如何在$\\LaTeX$中将超出文本宽度的浮动体（表格/图片）居中？这个问题我曾多次遇到过，但是没有一次记得怎么解决，每次都需要谷歌。今天也碰到了这个情况，因此将解决方案记录备忘。\n我看到的最优雅的解决方案出自$\\TeX$的StackExchange中的这篇回答。只要在浮动体内减少左右边距即可，也就是下列示例的第3-4行：\n1 2 3 4 5 6 7 \\begin{figure}[t] \\centering \\addtolength{\\leftskip}{-2cm} % increase (absolute) value if needed \\addtolength{\\rightskip}{-2cm} \\includegraphics[width=1.2\\textwidth]{image} \\caption{example}\\label{fig:example} \\end{figure} 效果如下图：\n","date":"2022-12-24T12:38:21+08:00","image":"https://files.furffisite.link/blogimg/20230511163659-9e52e93f3d72fae5e3a97613e49e921f-97fa4.jpg","permalink":"https://blog.furffisite.link/p/latex-widefloat-centering/","title":"如何将LaTeX内超出文本宽度的浮动体居中"},{"content":"前言 今天想给我这个博客加一张图片，但是把图片文件和博客的文章放在一起，内容管理比较麻烦，并且会增大git仓库的体积。因此我就想到了使用图床分流博客中的图片。\n在网上搜索了一些图床服务，发现国内免费的图床服务要么访问慢（因为源服务器在海外），要么不稳定（存在关站/被墙或者转为付费的可能），而国外著名的图床imgur在国内也处于半墙的状态。于是就想到了使用云计算厂商提供的OSS对象存储服务，虽然收费但是对于我这种有计划长时间运营下去的博客而言，图床的可靠性是最重要的。我可不想因为图站挂掉导致我在未来的某一天要重新找到再上传这些图片。\n在看了阿里云、腾讯云和华为云三家之后我选择了阿里云，因为阿里云有每月5GB存储和外网流量的的免费额度，请求费用也就每万次一毛钱，对于我这种刚开的小站而言，存储、流量和请求都不会很大（如果被攻击那就是另一回事了，还望您手下留情）。关于阿里云的定价详情可以查看阿里云的价格计算器。\n然后我就按照这篇博文的步骤搭建了图床，途中遇到了原文没有提及的许多问题，所以在这里记录一下完整的步骤、我遇到的问题与解决方案。\n主要操作流程 创建Bucket：开通OSS并创建Bucket。创建Bucket时选择海外的地域（如果在别的地域没有服务器的话，建议使用香港），存储类型选择标准存储即可，读写权限一定要选择私有，其余的附加服务按需启用（有的得加钱）。创建完成后可以向Bucket中上传一张图片作为测试图。\n配置访问权限：进入权限控制 -\u0026gt; Bucket授权策略面板，添加授权，配置如下图。\nIP字段填写的是CloudFlare的节点IP，列表如下（来自知乎专栏）：\n173.245.48.0/20,103.21.244.0/22,103.22.200.0/22,103.31.4.0/22, 141.101.64.0/18,108.162.192.0/18,190.93.240.0/20,188.114.96.0/20, 197.234.240.0/22,198.41.128.0/17,162.158.0.0/15,104.16.0.0/12, 172.64.0.0/13,131.0.72.0/22,103.21.244.0/22,103.22.200.0/22, 103.31.4.0/22,104.16.0.0/12,108.162.192.0/18,131.0.72.0/22, 141.101.64.0/18,162.158.0.0/15,172.64.0.0/13,173.245.48.0/20, 188.114.96.0/20,190.93.240.0/20,197.234.240.0/22,198.41.128.0/17 配置CDN：在CloudFlare的DNS管理面板添加CNAME记录，目标设为Bucket的域名（可以在Bucket的概览界面找到），代理状态设为已代理，否则CDN不起作用。 绑定域名：在阿里云的Bucket配置-\u0026gt;域名管理界面绑定你刚设置的域名，这时阿里云需要验证域名的所有权，按照其所说的在CloudFlare的DNS管理处添加指定TXT记录即可。\n创建并添加证书：在CloudFlare的配置面板的SSL/TLS-\u0026gt;源服务器处，选择创建证书。创建之后会告诉你源证书与私钥，这个界面暂时不要动。打开刚才在阿里云控制台绑定证书的界面，选择证书托管，并上传SSL证书，这时会打开SSL证书的界面，选择上传证书，并将CloudFlare给出的源证书和密钥复制到上传证书的对应字段处（证书名字随便设），然后确定。这时切换回上传SSL证书，应该就能在证书名称处看到刚刚设置的证书名字了（看不到的话重开一下这个界面试试），选中，然后点下方的上传即可。\n这时就已经可以通过你设置的域名访问刚才上传的测试图片了。假如测试图片filename.jpg存储在OSS的folder文件夹下，你设置的域名为image.example.org，则访问路径为https://image.example.org/folder/filename.jpg。\n安全性配置 跨域设置：在阿里云OSS的数据安全-\u0026gt;跨域设置中创建跨域规则，来源设置为你的网站的地址。为了能让网站在本地测试时也能正常展示图片，建议同时添加localhost:*与127.0.0.1:*。 防盗链设置：和跨域设置类似。不同之处在于Referer是包含请求协议的，所以类似于example.org或localhost:*等不包括协议的配置是无效的，需要改为https://example.com或*://localhost:*；需要注意的是*.example.org虽然是有效的，但是没有指定https协议，安全起见最好改为https://*.example.org。 PicGo 配置 PicGo是一款快速上传图片到图床，并自动复制图片URL到剪贴板的工具，你可以在Github的release页获取该程序。\n权限设置：在阿里云的权限控制 -\u0026gt; Bucket授权策略面板新增授权，配置如下： 如果当前没有RAM子帐号，请点击右上角头像-\u0026gt;访问控制，然后在左侧的身份管理-\u0026gt;用户处创建一个子帐号。创建完成后在子帐号的详情页创建AccessKey，得到AccessKey的KeyID与KeySecret，保留备用。\n上传配置：打开PicGo主界面，在图床设置-\u0026gt;阿里云OSS内填写对应的表单项。“KeyID”与“KeySecret”即刚才获取的子帐号AccessKey的KeyID与KeySecret，“设定Bucket”为Bucket的名称，“存储区域”为Bucket所在区域（与Bucket域名内的值统一，例如oss-cn-hongkong），自定义域名填写你设置的域名，其余两项按需填写即可。配置完成后点击确定并设为默认图床。\n注：如果你使用的桌面环境是KDE Plasma，可能需要在PicGo设置内打开“使用内置剪贴板上传”一项，否则无法正常从剪贴板直接上传图片。我使用的Linux发行版是KDE Neon，其它发行版/桌面环境/操作系统尚未测试。\n","date":"2022-12-20T01:48:09+08:00","image":"https://files.furffisite.link/blogimg/202212251307965.jpg","permalink":"https://blog.furffisite.link/p/imagebed-oss-conf/","title":"使用阿里云OSS存储服务+CloudFlare配置图床"}]