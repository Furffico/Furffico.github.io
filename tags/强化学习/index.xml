<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>强化学习 on Furffiblog</title><link>https://blog.furffisite.link/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link><description>Recent content in 强化学习 on Furffiblog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Thu, 18 May 2023 15:05:05 +0800</lastBuildDate><atom:link href="https://blog.furffisite.link/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>【RL学习笔记】Q-Learning Algorithm</title><link>https://blog.furffisite.link/p/q-learning/</link><pubDate>Thu, 18 May 2023 15:05:05 +0800</pubDate><guid>https://blog.furffisite.link/p/q-learning/</guid><description>&lt;img src="https://files.furffisite.link/blogimg/20230518191643-b6c8849bfb006d40abef7203bb5e15a0-18b9a.jpg" alt="Featured image of post 【RL学习笔记】Q-Learning Algorithm" />&lt;h2 id="算法">算法&lt;/h2>
&lt;p>Q学习算法（Q-Learning Algorithm）的思路比较简单：使用Q函数记录每一个状态时每一个动作（action）的期望最大总回报（reward），即Q值。在推理时贪心地选择当前状态Q值最大的动作，从而达到最大化期望总回报的目的。当问题的状态与动作均为离散时，Q函数可以使用表格记录，这个表格也称为Q表格（Q-Table）。&lt;/p>
&lt;p>设：&lt;/p>
&lt;ul>
&lt;li>问题的状态空间为 $S = {1,~2,~\ldots, m}$；&lt;/li>
&lt;li>问题的动作空间为 $A = {1,~2,~\ldots, n}$；&lt;/li>
&lt;li>探索阈值为 $\epsilon\in [0,1]$（推理时 $\epsilon = 0$）；&lt;/li>
&lt;li>学习率 $\eta\in [0,1]$；&lt;/li>
&lt;li>回报衰减率 $\gamma\in [0,1]$。&lt;/li>
&lt;/ul>
&lt;p>则Q学习算法的流程如下：&lt;/p>
&lt;ol>
&lt;li>初始化Q表格为零矩阵 $Q=O_{m\times n}$；&lt;/li>
&lt;li>设初始时间 $t = 0$，状态为 $s_t = s_0$；&lt;/li>
&lt;li>选择动作 $a_t$：取随机数 $r\in[0,1]$，若 $r&amp;lt;\epsilon$，则当前为探索阶段，从 $A$ 随机选取一个动作；否则 $a_t = \argmax_{j\in A} Q_{s_t, j}$；&lt;/li>
&lt;li>与环境交互，执行动作 $a_t$，并获得状态 $s_{t+1}$、回报 $r_t$；&lt;/li>
&lt;li>按照 &lt;a class="link" href="https://en.wikipedia.org/wiki/Bellman_equation" target="_blank" rel="noopener"
>Bellman 方程&lt;/a> 更新Q表格：
$$Q_{s_t, a_t} \leftarrow (1-\eta) Q_{s_t, a_t} + \eta(\gamma\max_{j\in A} Q_{s_{t+1},j} + r_t),$$
其中 $\max Q_{s_{t+1},j}$ 为 转移后的状态 $s_{t+1}$ 下最大的Q值，加上 $r_t$，组成转移前状态 $s_t$ 下 $a_t$ 操作的Q值。将其与原来的 $Q_{s_t, a_t}$ 加权平均就得到了更新后的Q值。&lt;/li>
&lt;li>时间 $t\leftarrow t+1$，回到第 3 步。&lt;/li>
&lt;/ol>
&lt;h2 id="实现">实现&lt;/h2>
&lt;h3 id="frozen-lake">Frozen Lake&lt;/h3>
&lt;p>以 &lt;a class="link" href="https://gymnasium.farama.org/" target="_blank" rel="noopener"
>Gym&lt;/a> 的 &lt;a class="link" href="https://gymnasium.farama.org/environments/toy_text/frozen_lake/" target="_blank" rel="noopener"
>Frozen Lake&lt;/a> 环境为例，其状态空间与动作空间都是离散的，因此适合使用Q表格。这个环境共有 $4^2=16$ 种状态，4种动作，分别对应上下左右四个移动方向。环境中&lt;code>slippery&lt;/code>引入的不确定性太大（每次行动只有$1/3$的概率能真正前往指定的方向），因此这里创建环境时&lt;code>is_slippery&lt;/code>一项设为&lt;code>False&lt;/code>。&lt;/p>
&lt;p>实现如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">gymnasium&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">gym&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">random&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">random&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">tqdm&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">tqdm&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">env&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">gym&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">make&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;FrozenLake-v1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">map_name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;4x4&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">is_slippery&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">n_actions&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">env&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">action_space&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">env&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">observation_space&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">n&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">lr&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">gamma&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">total&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1000&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Q_table&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">n_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n_actions&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">state&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">info&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">env&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reset&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">total&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 选择动作&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">epsilon&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">total&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">random&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">epsilon&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="c1"># 探索&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">action&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">env&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">action_space&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sample&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">action&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Q_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">state&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">org_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">state&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">state&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">reward&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">terminated&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">truncated&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">info&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">env&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">step&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">action&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">state&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">n_states&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="c1"># 抵达终点，获得奖励&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reward&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">20&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">elif&lt;/span> &lt;span class="n">terminated&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="c1"># 掉进洞里，给予惩罚&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reward&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="c1"># 让agent尽快抵达终点&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reward&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 更新Q值&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Q_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">org_state&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">action&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">((&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">Q_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">org_state&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">action&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">+&lt;/span> &lt;span class="n">lr&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">reward&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">gamma&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">Q_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">state&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max&lt;/span>&lt;span class="p">()))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">terminated&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="n">truncated&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">state&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">info&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">env&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reset&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>原环境提供的回报函数所含的信息较少（仅在抵达终点时为1，其余情况均为0），不利于算法的收敛，因此我在实现中重新设计了回报函数。
此外，&lt;/p>
&lt;p>训练时每轮的t与该轮获得的总reward的折线图如下：&lt;/p>
&lt;p>&lt;img src="https://files.furffisite.link/blogimg/20230518182030-919e874eac177ca3e790e7ee6f3046de-a01ac.png"
loading="lazy"
>&lt;/p>
&lt;p>推理时算法选择的路线如下图，确实是最优路线之一。&lt;/p>
&lt;p>&lt;img src="https://files.furffisite.link/blogimg/20230518182047-803e364c89da138e7326b34667db82f2-057ca.gif"
loading="lazy"
>&lt;/p>
&lt;p>除了 Frozen Lake，这个方法也可以用来求解 Gym 提供的 Cliff Walking 和 Taxi。但是，随着问题规模的增大，训练步数也需要相应地增加。&lt;/p>
&lt;h3 id="cliff-walking">Cliff Walking&lt;/h3>
&lt;p>这个环境有48种状态和4种动作，因此Q表格内共有192项。以下是训练了 10,000 步的结果（平均每项52步）：&lt;/p>
&lt;p>&lt;img src="https://files.furffisite.link/blogimg/20230518183755-c54395a25e87cd173c7c9b80b5e60d30-35491.gif"
loading="lazy"
>&lt;/p>
&lt;h3 id="taxi">Taxi&lt;/h3>
&lt;p>这个环境有500种状态和6种动作，对应Q表格的3000项。以下是训练了 200,000 步的结果（平均每项67步）：&lt;/p>
&lt;p>&lt;img src="https://files.furffisite.link/blogimg/20230518185029-6e6e54510ded0b9180a5d99c54ee3756-689cf.gif"
loading="lazy"
>&lt;/p>
&lt;h2 id="分析">分析&lt;/h2>
&lt;h3 id="优点">优点&lt;/h3>
&lt;ul>
&lt;li>算法简单，易于理解和实现。&lt;/li>
&lt;/ul>
&lt;h3 id="局限性">局限性&lt;/h3>
&lt;ul>
&lt;li>基于Q表格的Q学习算法只能处理输入输出都是离散的问题；&lt;/li>
&lt;li>基于Q表格的Q学习算法不能跨实例学习，即在遇到新的问题实例时，需要从0开始重新探索；&lt;/li>
&lt;li>基于Q表格的Q学习算法无法处理训练过程中没有见过的状态；&lt;/li>
&lt;li>当状态空间或动作空间很大时，Q表格的规模也会很大，从而需要更长的学习时间。&lt;/li>
&lt;/ul>
&lt;p>上述问题可以通过引入神经网络缓解，即深度Q学习算法（Deep Q-Learning Algorithm）。&lt;/p></description></item></channel></rss>