<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Div-S3 is a non-learning approach to retrieve in-context exemplars from a large unlabeled dataset."><title>【读论文】An End-to-End Submodular Framework for Data-Efficient In-Context Learning</title>
<link rel=canonical href=https://blog.furffisite.link/p/read-papers/div-s3/><link rel=stylesheet href=/scss/style.min.1d9acc6d7417da29e26c03e15654a8a97a1eb01eaece9c154c7b2b3721fec799.css><meta property='og:title' content="【读论文】An End-to-End Submodular Framework for Data-Efficient In-Context Learning"><meta property='og:description' content="Div-S3 is a non-learning approach to retrieve in-context exemplars from a large unlabeled dataset."><meta property='og:url' content='https://blog.furffisite.link/p/read-papers/div-s3/'><meta property='og:site_name' content='Furffiblog'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='机器学习'><meta property='article:tag' content='读论文'><meta property='article:tag' content='组合优化'><meta property='article:tag' content='上下文学习'><meta property='article:published_time' content='2024-07-08T12:33:03+08:00'><meta property='article:modified_time' content='2024-07-08T12:33:03+08:00'><meta property='og:image' content='https://files.furffisite.link/blogimg/20240709193826-409d3c5c4adf797af0b0d55992852fce-5194f.jpg'><meta name=twitter:title content="【读论文】An End-to-End Submodular Framework for Data-Efficient In-Context Learning"><meta name=twitter:description content="Div-S3 is a non-learning approach to retrieve in-context exemplars from a large unlabeled dataset."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://files.furffisite.link/blogimg/20240709193826-409d3c5c4adf797af0b0d55992852fce-5194f.jpg'><link rel=preconnect href=https://files.furffisite.link crossorigin><link rel=preconnect href=https://gfonts.aby.pub crossorigin><script async src=https://analytics.umami.is/script.js data-website-id=b3257363-e219-4eac-b59d-d75b9aba64b5></script><link href=https://files.furffisite.link/fontcache/NotoSansSC_Mulish_remote.min.css rel=preload as=style><link href=https://gfonts.aby.pub/s/mulish/v13/1Ptyg83HX_SGhgqO0yLcmjzUAuWexZNR8aevGw.woff2 rel=preload as=font><style>:root{--zh-font-family:"Noto Sans SC", "PingFang SC", "Hiragino Sans GB", "Droid Sans Fallback", "Microsoft YaHei";--base-font-family:"Mulish", var(--sys-font-family), var(--zh-font-family), sans-serif}</style></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu8a7a5e24c6afd9e1fde1d549095c0023_459346_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🗑️</span></figure><div class=site-meta><h1 class=site-name><a href=/>Furffiblog</a></h1><h2 class=site-description>随便写写</h2></div></header><ol class=menu-social><li><a href=https://github.com/furffico target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li><a href=/gallery/><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32"><g fill="none"><path d="M3 7a4 4 0 014-4h14a4 4 0 014 4v14a4 4 0 01-4 4H7a4 4 0 01-4-4V7zm4-2A2 2 0 005 7v14a2 2 0 00.077.552l7.32-6.914c.9-.85 2.306-.85 3.205.0l7.32 6.914A2 2 0 0023 21V7a2 2 0 00-2-2H7zm0 18h14c.166.0.328-.02.482-.058l-7.253-6.85a.333.333.0 00-.458.0l-7.253 6.85c.155.038.316.058.482.058zm11.5-11a1.5 1.5.0 100-3 1.5 1.5.0 000 3zm-8 16.5a3.998 3.998.0 01-3.465-2H21a5.5 5.5.0 005.5-5.5V7.035c1.196.692 2 1.984 2 3.465V21A7.5 7.5.0 0121 28.5H10.5z" fill="currentcolor"/></g></svg>
<span>相册</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>文章归档</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#basic-information>Basic Information</a></li><li><a href=#the-problem-to-solve>The Problem to Solve</a></li><li><a href=#the-proposed-algorithm>The Proposed Algorithm</a><ol><li><a href=#exemplar-annotation>Exemplar Annotation</a></li><li><a href=#exemplar-retrieval>Exemplar Retrieval</a><ol><li><a href=#phase-1-of-s3>Phase 1 of S3</a></li><li><a href=#phase-2-of-s3>Phase 2 of S3</a></li></ol></li></ol></li><li><a href=#comments>Comments</a></li><li><a href=#references>References</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/read-papers/div-s3/><img src=https://files.furffisite.link/blogimg/20240709193826-409d3c5c4adf797af0b0d55992852fce-5194f.jpg loading=lazy alt="Featured image of post 【读论文】An End-to-End Submodular Framework for Data-Efficient In-Context Learning"></a><div class=image-copyright><span>Photo by <a class=link href=https://unsplash.com/@rutzsepp target=_blank rel=noopener>Sepp Rutz</a> on <a class=link href=https://unsplash.com/photos/waterfall-between-trees-covered-with-snow-aF5Fpc5nY5s target=_blank rel=noopener>Unsplash</a></span></div></div><div class=article-details><header class=article-category><a href=/categories/%E7%AC%94%E8%AE%B0/>笔记</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/read-papers/div-s3/>【读论文】An End-to-End Submodular Framework for Data-Efficient In-Context Learning</a></h2><h3 class=article-subtitle>Div-S3 is a non-learning approach to retrieve in-context exemplars from a large unlabeled dataset.</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 08, 2024</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 4 分钟</time></div></footer></div></header><section class=article-content><p>This blog post is completely written in English, just for practicing my English writing skills. Please let me know if there is any suggestions.</p><hr><h2 id=basic-information>Basic Information</h2><ul><li>Title： <strong>An End-to-End Submodular Framework for Data-Efficient In-Context Learning</strong> <sup><a id=ref-cite1-1 href=#cite1>[1]</a></sup></li><li>Authors： Lilly Kumari, Shengjie Wang, Arnav Das, Tianyi Zhou, Jeff Bilmes</li><li>Conference： <a class=link href=https://2024.naacl.org/ target=_blank rel=noopener>NAACL 2024</a></li><li>Open Access： <a class=link href=https://aclanthology.org/2024.findings-naacl.209 target=_blank rel=noopener>https://aclanthology.org/2024.findings-naacl.209</a></li></ul><h2 id=the-problem-to-solve>The Problem to Solve</h2><p><strong>The problem of annotating, selecting and ordering in-context exemplars for Large Language Models (LLMs).</strong></p><p>The In-Context Learning (ICL) performance of LLMs is largely affected by the selection and ordering of in-context exemplars, which makes it necessary to develop a methodology to select the in-context exemplars according to the query.</p><h2 id=the-proposed-algorithm>The Proposed Algorithm</h2><p>In reality, the most of the data we have are unannotated data, <em>i.e.</em> queries without answers, some recent methods <sup><a id=ref-cite2-1 href=#cite2>[2]</a></sup> suggest to select and annotate a subset from an unannotated huge dataset
$\mathcal X_\mathrm{unlabeled}$ to form a small annotated dataset $\mathcal X_\mathrm{labeled}$, and to choose the in-context exemplars from this subset during evaluation. This passage followed this paradigm, and proposed <strong>Div-S3</strong>, a two-stage data-efficient learning-free framework for exemplar selection.</p><ul><li>The first stage (<strong>Div</strong>): Exemplar Annotation, from $\mathcal X_\mathrm{unlabeled}$ to $\mathcal X_\mathrm{labeled}$.</li><li>The second stage (<strong>S3</strong>): Exemplar Retrieval, from $\mathcal X_\mathrm{labeled}$ and query set $Q$ to get in-context exemplars $\mathcal D_\mathrm{context}$.</li></ul><h3 id=exemplar-annotation>Exemplar Annotation</h3><p><strong>Problem Definition.</strong>
The first stage of <em>Div-S3</em>, as mentioned above, selects from unannotated data $\mathcal X_\mathrm{unlabeled}$ and let <em>homo sapiens</em> do the annotations to make an informative and diverse subset $\mathcal X_\mathrm{labeled}$, with the constraint of $|\mathcal X_\mathrm{labeled}| \ll |\mathcal X_\mathrm{unlabeled}|$. This is a process similar to one iteration of pool-based active learning. With the objective of diversity and reducing redundancy, this can also be formulated as a set optimization problem <sup><a id=ref-cite3-1 href=#cite3>[3]</a></sup> as follows:
$$\max_{A\subset \mathcal X_\mathrm{unlabeled},|A|\le k} f(A),$$
where $k$ is a hyperparameter representing the annotation budget, $f:~2^{\mathcal X_\mathrm{unlabeled}}\rightarrow\mathbb R$ is a submodular function mapping all subsets of $\mathcal X_\mathrm{unlabeled}$ to the respective score of each subset. The higher the score, the better the subset is.</p><blockquote><p>The submodular function must satisfy the properties of monotone and decreasing marginal profit, <em>i.e.</em>, respectively,
$$\begin{aligned}\forall A\subseteq T, \quad&amp;f(A) \le f(T), \\ \forall A\subset T,~ \forall x\not\in T, \quad&amp;f(\{x\}|A) \ge f(\{x\}|T), \end{aligned}$$
where $f(\{x\}|T) = f(\{x\}\cup T) - f(T)$ is the marginal profit of adding $\{x\}$ to $T$.</p></blockquote><p>The authors set the submodular function to be <em>facility location</em>, which is defined as
$$f(A) = \sum_{s_i\in\mathcal X_\mathrm{unlabeled}}\max_{s_j\in A}\mathrm{sim}(s_i,s_j),$$
where $\mathrm{sim}(\cdot,\cdot)$ denotes the cosine similarity of two queries&rsquo; embeddings, generated by sentence-BERT <sup><a id=ref-cite4-1 href=#cite4>[4]</a></sup>.</p><p><strong>Intuitive Interpretation.</strong>
This problem can be intuitively and geometrically interpreted as minimizing the sum of all distances between each node and its nearest selected nodes, which is a k-medoids problem and is pretty similar to k-means clustering.</p><p><strong>Solution.</strong>
The authors of this paper use a greedy algorithm proposed by Nemhauser <em>et al.</em> <sup><a id=ref-cite5-1 href=#cite5>[5]</a></sup>, called Greedy Submodular Maximization (GSM). Its fundamental idea is to greedily select the item with the maximum marginal profit for $k$ iterations, <em>i.e.</em>
$$A\leftarrow A\cup \{\argmax_{v\in V, v\not\in A} f(A\cup \{v\}) - f(A) \}.$$
Considering the time spent in calculating $f$, this is an algorithm with the time complexity of $O((nk)^2)$. The paper says it&rsquo;s $O(n^2+nk)$ but I beg to differ as it seems to have ignored the time complexity of calculating $f$, which is $\Theta(nm)$ with pre-calculated similarity matrix, where $m=0,1,\ldots,k-1$ in the iterations. But I agree with the authors that the algorithm can be accelerated by techniques like caching and priority queues, <em>etc.</em></p><h3 id=exemplar-retrieval>Exemplar Retrieval</h3><p>Exemplar retrieval is intended to find the best in-context exemplars $\mathcal D_\mathrm{context}$ for the given query set $Q$ from $\mathcal X_\mathrm{labeled}$. Previous similarity-based methods usually yield in-context exemplars with redundant information. To reduce such redundancy, the authors formalize exemplar retrieval as a conditional submodular subset selection problem. The purpose of this stage is to &ldquo;obtain a set of exemplars that are not only relevant to the test query but also encompass diverse aspects crucial for aiding the LLM in the target task&rdquo; <sup><a id=ref-cite1-2 href=#cite1>[1]</a></sup>. They come up with a two-phase method called Submodular Span Summarization (S3), which was published prior to this paper <sup><a id=ref-cite6-1 href=#cite6>[6]</a></sup>.</p><h4 id=phase-1-of-s3>Phase 1 of S3</h4><p><strong>Problem Definition.</strong>
This phase targets to select a relatively large subset relevant to the query set $Q$, but might be redundant.
The original problem might be difficult to solve, so the paper considered solving the dual problem of it:
$$\min_{A\subseteq V - Q,|A|\ge k_1}f(A|Q),$$
which is a cardinality-constrained submodular minimization problem. The authors use $m_Q(A) = \sum_{a\in A}f(a|Q)$ to approximate $f(A|Q)$, which is an upper bound of $f(A|Q)$.</p><p><strong>Solution.</strong>
Although the paper doesn&rsquo;t state it clearly, I think the algorithm to solve this problem, given the approximation, is to select $k_1$ exemplars with minimal values of $f(a|Q)$ from $A$. And this algorithm matches the time complexity $O(k+k\log k_1)$ stated in Appendix B, if ignoring the time for calculating $f$.</p><h4 id=phase-2-of-s3>Phase 2 of S3</h4><p>This stage is intended to select the most representative exemplars from the result of phase 1, and is mathematically the same as <a class=link href=#exemplar-annotation>Exemplar Annotation</a>:
$$\max_{A\subset A_Q,|A|\le k_2} f(A),$$
where $A_Q$ is the set of selected exemplars from phase 1.
Optionally, we can apply a knapsack constraint on the problem, and solve it with a modified version of GSM.</p><h2 id=comments>Comments</h2><p>The authors proposed a learning-free framework named <strong>Div-S3</strong> to select in-context exemplars from unlabeled datasets, and evaluated its effectiveness with ablation experiments across 7 Natural Language Processing (NLP) tasks and 5 LLMs. Although the algorithm doesn&rsquo;t take ordering into consideration, the paper proves its insensitiveness to the order of exemplars.</p><p>However, from my perspective, I still have some problems related to the paper:</p><ul><li>In Exemplar Retrieval stage, what&rsquo;s the meaning of computing $f(Q)$ against all unlabeled data $\mathcal X_\mathrm{unlabeled}$. Given that $\mathcal X_\mathrm{labeled}$ is a representative subset of $\mathcal X_\mathrm{unlabeled}$, would it be computationally better to calculate $f(Q)$ and $f(a|Q)$ only against $\mathcal X_\mathrm{labeled}\cup Q$?</li><li>What&rsquo;s the purpose of dealing with the queries as a whole (query set $Q$), instead of retrieving the best exemplars for each query $q$?</li><li>The experiments didn&rsquo;t cover the comparison of Div-S3 with some recent algorithms, especially the learning-based ones.</li><li>Will the algorithm averagely perform better if we handcraft a rule to arrange the selected exemplars? Also, in Figure 3, the variances do not seem to be small.</li></ul><h2 id=references>References</h2><style>.bibliography{display:table;font-size:medium;line-height:normal}.bib-item{display:table-row}.bib-item>:first-child{display:table-cell;padding-right:.5em;font-weight:700;text-align:right}.bib-item>:last-child{display:table-cell;padding-bottom:.5ex}</style><div class=bibliography><div id=cite1 class=bib-item><span>[1]</span>
<span>L. Kumari, S. Wang, A. Das, T. Zhou, and J. Bilmes, “An End-to-End Submodular Framework for Data-Efficient In-Context Learning,” in Findings of the Association for Computational Linguistics: NAACL 2024, K. Duh, H. Gomez, and S. Bethard, Eds., Mexico City, Mexico: Association for Computational Linguistics, Jun. 2024, pp. 3293–3308. Accessed: Jul. 02, 2024. [Online]. Available: <a class=link href=https://aclanthology.org/2024.findings-naacl.209 target=_blank rel=noopener>https://aclanthology.org/2024.findings-naacl.209</a><a href=#ref-cite1-1>⤶</a><a href=#ref-cite1-2>⤶</a></span></div><div id=cite2 class=bib-item><span>[2]</span>
<span>H. Su et al., “Selective Annotation Makes Language Models Better Few-Shot Learners,” presented at the The Eleventh International Conference on Learning Representations, Sep. 2022. Accessed: May 30, 2024. [Online]. Available: <a class=link href="https://openreview.net/forum?id=qY1hlv7gwg" target=_blank rel=noopener>https://openreview.net/forum?id=qY1hlv7gwg</a><a href=#ref-cite2-1>⤶</a></span></div><div id=cite3 class=bib-item><span>[3]</span>
<span>S. C. H. Hoi, R. Jin, J. Zhu, and M. R. Lyu, “Batch mode active learning and its application to medical image classification,” in Proceedings of the 23rd international conference on Machine learning, in ICML ’06. New York, NY, USA: Association for Computing Machinery, Jun. 2006, pp. 417–424. doi: 10.1145/1143844.1143897.<a href=#ref-cite3-1>⤶</a></span></div><div id=cite4 class=bib-item><span>[4]</span>
<span>N. Reimers and I. Gurevych, “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.” arXiv, Aug. 27, 2019. Accessed: Jul. 08, 2024. [Online]. Available: <a class=link href=http://arxiv.org/abs/1908.10084 target=_blank rel=noopener>http://arxiv.org/abs/1908.10084</a><a href=#ref-cite4-1>⤶</a></span></div><div id=cite5 class=bib-item><span>[5]</span>
<span>G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher, “An analysis of approximations for maximizing submodular set functions—I,” Mathematical programming, vol. 14, pp. 265–294, 1978.<a href=#ref-cite5-1>⤶</a></span></div><div id=cite6 class=bib-item><span>[6]</span>
<span>L. Kumari and J. Bilmes, “Submodular Span, with Applications to Conditional Data Summarization,” Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 14, Art. no. 14, May 2021, doi: 10.1609/aaai.v35i14.17465.<a href=#ref-cite6-1>⤶</a></span></div></div></section><footer class=article-footer><section class=article-tags><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a>
<a href=/tags/%E8%AF%BB%E8%AE%BA%E6%96%87/>读论文</a>
<a href=/tags/%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96/>组合优化</a>
<a href=/tags/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>上下文学习</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>本文基于<a class=link href=https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode.zh-Hans target=_blank rel=noopener><u>CC BY-NC-SA 4.0 协议</u></a>发布。本文<strong>不可商用</strong>，转载时请<strong>注明出处</strong>并使用<strong>相同协议</strong>。</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/read-papers/utsp/><div class=article-image><img src=https://files.furffisite.link/blogimg/20240312193735-df25ede3714c69d2ea8b42ba9c39f7c9-755f3.jpg loading=lazy data-key=read-papers/utsp data-hash=https://files.furffisite.link/blogimg/20240312193735-df25ede3714c69d2ea8b42ba9c39f7c9-755f3.jpg></div><div class=article-details><h2 class=article-title>【读论文】Unsupervised Learning for Solving the Travelling Salesman Problem</h2></div></a></article><article class=has-image><a href=/p/deep-q-learning/><div class=article-image><img src=https://files.furffisite.link/blogimg/20230822110113-5ef18ab6cfb63d5a730bbd8e077231e7-6ba69.jpg loading=lazy data-key=deep-Q-learning data-hash=https://files.furffisite.link/blogimg/20230822110113-5ef18ab6cfb63d5a730bbd8e077231e7-6ba69.jpg></div><div class=article-details><h2 class=article-title>【RL学习笔记】深度Q学习算法与经验回放</h2></div></a></article><article class=has-image><a href=/p/q-learning/><div class=article-image><img src=https://files.furffisite.link/blogimg/20230518191643-b6c8849bfb006d40abef7203bb5e15a0-18b9a.jpg loading=lazy data-key=Q-learning data-hash=https://files.furffisite.link/blogimg/20230518191643-b6c8849bfb006d40abef7203bb5e15a0-18b9a.jpg></div><div class=article-details><h2 class=article-title>【RL学习笔记】Q学习算法</h2></div></a></article><article class=has-image><a href=/p/windows-setup/><div class=article-image><img src=https://files.furffisite.link/blogimg/20240707215654-5c276c9ae1c98340521bf56811f4b765-87c36.jpg loading=lazy data-key=windows-setup data-hash=https://files.furffisite.link/blogimg/20240707215654-5c276c9ae1c98340521bf56811f4b765-87c36.jpg></div><div class=article-details><h2 class=article-title>个人常用的新 Windows 11 系统的配置过程</h2></div></a></article><article class=has-image><a href=/p/applemusic-transfer/><div class=article-image><img src=https://files.furffisite.link/blogimg/20240523203808-9f4a7d937672d53fbc1a7174037c6f64-581fc.jpg loading=lazy data-key=applemusic-transfer data-hash=https://files.furffisite.link/blogimg/20240523203808-9f4a7d937672d53fbc1a7174037c6f64-581fc.jpg></div><div class=article-details><h2 class=article-title>Apple Music 曲库迁移</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=Furffico/Furffico.github.io data-repo-id=R_kgDOIs6c3Q data-category=General data-category-id=DIC_kwDOIs6c3c4CTWXf data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=en crossorigin=anonymous async></script><script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"light":"dark_dimmed")}})()</script><footer class=site-footer><section class=copyright>&copy;
2022 -
2024 Furffiblog</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.26.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://files.furffisite.link/fontcache/NotoSansSC_Mulish_remote.min.css",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>