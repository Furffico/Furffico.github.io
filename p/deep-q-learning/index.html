<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Deep Q-Learning Algorithm & Experience Replay"><title>【RL学习笔记】深度Q学习算法与经验回放</title>
<link rel=canonical href=https://blog.furffisite.link/p/deep-q-learning/><link rel=stylesheet href=/scss/style.min.1d9acc6d7417da29e26c03e15654a8a97a1eb01eaece9c154c7b2b3721fec799.css><meta property='og:title' content="【RL学习笔记】深度Q学习算法与经验回放"><meta property='og:description' content="Deep Q-Learning Algorithm & Experience Replay"><meta property='og:url' content='https://blog.furffisite.link/p/deep-q-learning/'><meta property='og:site_name' content='Furffiblog'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='机器学习'><meta property='article:tag' content='强化学习'><meta property='article:published_time' content='2023-08-22T10:51:10+08:00'><meta property='article:modified_time' content='2023-08-22T10:51:10+08:00'><meta property='og:image' content='https://files.furffisite.link/blogimg/20230822110113-5ef18ab6cfb63d5a730bbd8e077231e7-6ba69.jpg'><meta name=twitter:title content="【RL学习笔记】深度Q学习算法与经验回放"><meta name=twitter:description content="Deep Q-Learning Algorithm & Experience Replay"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://files.furffisite.link/blogimg/20230822110113-5ef18ab6cfb63d5a730bbd8e077231e7-6ba69.jpg'><link rel=preconnect href=https://files.furffisite.link crossorigin><link rel=preconnect href=https://gfonts.aby.pub crossorigin><script async src=https://analytics.umami.is/script.js data-website-id=b3257363-e219-4eac-b59d-d75b9aba64b5></script><link href=https://files.furffisite.link/fontcache/NotoSansSC_Mulish_remote.min.css rel=preload as=style><link href=https://gfonts.aby.pub/s/mulish/v13/1Ptyg83HX_SGhgqO0yLcmjzUAuWexZNR8aevGw.woff2 rel=preload as=font><style>:root{--zh-font-family:"Noto Sans SC", "PingFang SC", "Hiragino Sans GB", "Droid Sans Fallback", "Microsoft YaHei";--base-font-family:"Mulish", var(--sys-font-family), var(--zh-font-family), sans-serif}</style></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu8a7a5e24c6afd9e1fde1d549095c0023_459346_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🗑️</span></figure><div class=site-meta><h1 class=site-name><a href=/>Furffiblog</a></h1><h2 class=site-description>随便写写</h2></div></header><ol class=menu-social><li><a href=https://github.com/furffico target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li><a href=/gallery/><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32"><g fill="none"><path d="M3 7a4 4 0 014-4h14a4 4 0 014 4v14a4 4 0 01-4 4H7a4 4 0 01-4-4V7zm4-2A2 2 0 005 7v14a2 2 0 00.077.552l7.32-6.914c.9-.85 2.306-.85 3.205.0l7.32 6.914A2 2 0 0023 21V7a2 2 0 00-2-2H7zm0 18h14c.166.0.328-.02.482-.058l-7.253-6.85a.333.333.0 00-.458.0l-7.253 6.85c.155.038.316.058.482.058zm11.5-11a1.5 1.5.0 100-3 1.5 1.5.0 000 3zm-8 16.5a3.998 3.998.0 01-3.465-2H21a5.5 5.5.0 005.5-5.5V7.035c1.196.692 2 1.984 2 3.465V21A7.5 7.5.0 0121 28.5H10.5z" fill="currentcolor"/></g></svg>
<span>相册</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>文章归档</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#深度q学习算法>深度Q学习算法</a><ol><li><a href=#理论>理论</a></li><li><a href=#实现>实现</a></li></ol></li><li><a href=#经验回放>经验回放</a><ol><li><a href=#理论-1>理论</a></li><li><a href=#实现-1>实现</a></li></ol></li><li><a href=#参考文献>参考文献</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/deep-q-learning/><img src=https://files.furffisite.link/blogimg/20230822110113-5ef18ab6cfb63d5a730bbd8e077231e7-6ba69.jpg loading=lazy alt="Featured image of post 【RL学习笔记】深度Q学习算法与经验回放"></a></div><div class=article-details><header class=article-category><a href=/categories/%E7%AC%94%E8%AE%B0/>笔记</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/deep-q-learning/>【RL学习笔记】深度Q学习算法与经验回放</a></h2><h3 class=article-subtitle>Deep Q-Learning Algorithm & Experience Replay</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Aug 22, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 6 分钟</time></div></footer></div></header><section class=article-content><h2 id=深度q学习算法>深度Q学习算法</h2><h3 id=理论>理论</h3><p>深度Q学习算法（Deep Q-Learning Algorithm）是将 Q 表格替换为神经网络的 Q 学习算法，由 DeepMind 的 Mnih et al. <sup><a id=ref-cite1-1 href=#cite1>[1]</a></sup><sup><a id=ref-cite2-1 href=#cite2>[2]</a></sup>提出。
Q表格本质上是一个函数 $f: S\times A \rightarrow \mathbb{R}$，我们自然也可以使用神经网络构造这个函数，让它可以处理连续的状态和动作。此外，使用神经网络还有一个好处：我们可以向神经网络输入实例信息 $m\in M$ ，使之可以跨实例学习函数 $f: M\times S\times A\rightarrow \mathbb{R}$ 。也就是说，Agent可以将其在实例 $m_1,<del>m_2,</del>\ldots~m_n$上 学习到的经验迁移到未曾见过的实例 $m_{n+1}$ 上，增强模型的泛化性能，减少其探索新实例所需的时间。</p><p>网络更新方程的设计
（以 <a class=link href=https://en.wikipedia.org/wiki/Bellman_equation target=_blank rel=noopener>Bellman 方程</a> 为基础）：
$$Q(s_t, a_t) \leftarrow (1-\eta) Q(s_t, a_t) + \eta(\gamma\max_{j\in A} Q(s_{t+1}, j) + r_t)$$</p><p>求更新前与更新后的差分：
$$\Delta Q(s_t, a_t) = -\eta Q(s_t, a_t) + \eta(\gamma\max_{j\in A} Q(s_{t+1},j) + r_t)$$</p><p>即：
$$\Delta Q(s_t, a_t) = \eta(\gamma\max_{j\in A} Q(s_{t+1},j) + r_t - Q(s_t, a_t))$$</p><p>在理想情况下充分训练时，应当有
$$\lim_{t\rightarrow \infty}\Delta Q(s_t, a_t) = 0$$</p><p>也就是说，训练的目标应当是最小化 $\Delta Q(s_t, a_t)$，即目标函数为：
$$L(\theta) = \mathrm{MSE}(Q(s_t, a_t),~\gamma\max_{j\in A} Q(s_{t+1},j) + r_t)$$
其中的 $\mathrm{MSE}$ 也可以替换为其它的损失函数。</p><h3 id=实现>实现</h3><p>下面以<a class=link href=https://gymnasium.farama.org/environments/classic_control/cart_pole/ target=_blank rel=noopener>CartPole-v1环境</a>为例编写训练程序。</p><p>引入相关的库以及定义一些超参数：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>random</span> <span class=kn>import</span> <span class=n>random</span><span class=p>,</span> <span class=n>randint</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>gymnasium</span> <span class=k>as</span> <span class=nn>gym</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tqdm</span> <span class=kn>import</span> <span class=n>tqdm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>n_actions</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>n_states</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=n>lr</span> <span class=o>=</span> <span class=mf>3e-4</span>
</span></span><span class=line><span class=cl><span class=n>discount</span> <span class=o>=</span> <span class=mf>0.95</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=n>epochs</span> <span class=o>=</span> <span class=mi>5000</span>
</span></span></code></pre></td></tr></table></div></div><p>定义神经网络，这里定义了一个简单的三层神经网络，其中输出层没有添加激活函数是因为激活函数会限制网络的值域至 $R_{act}$ ，设Q函数的值域是 $R_Q$，$R_Q\nsubseteq R_{act}$ 时损失函数难以收敛，影响训练效果：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Net</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>norm_vector</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=mf>0.21</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_feats</span> <span class=o>=</span> <span class=n>n_states</span><span class=p>,</span> <span class=n>out_feats</span> <span class=o>=</span> <span class=n>n_actions</span><span class=p>,</span> <span class=n>hidden</span> <span class=o>=</span> <span class=mi>32</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>in_feats</span><span class=p>,</span> <span class=n>hidden</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>SiLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden</span><span class=p>,</span> <span class=n>hidden</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>SiLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden</span><span class=p>,</span> <span class=n>out_feats</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>state</span><span class=o>/</span><span class=bp>self</span><span class=o>.</span><span class=n>norm_vector</span> <span class=c1># 归一化</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y</span>
</span></span></code></pre></td></tr></table></div></div><p>定义网络、优化器与损失函数：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>net</span> <span class=o>=</span> <span class=n>Net</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>net</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>,</span> <span class=n>amsgrad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>SmoothL1Loss</span><span class=p>()</span> <span class=c1># 发现L1的效果比L2要好</span>
</span></span></code></pre></td></tr></table></div></div><p>训练过程（原环境提供的 reward 恒为 1，信息太少，因此这里改用自定义的 reward，在倾角过大或位置过远时进行惩罚）：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>env</span> <span class=o>=</span> <span class=n>gym</span><span class=o>.</span><span class=n>make</span><span class=p>(</span><span class=s2>&#34;CartPole-v1&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>state</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=n>tqdm</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=n>epsilon</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>t</span> <span class=o>/</span> <span class=n>epochs</span>  <span class=c1># 动态调整epsilon</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>batch_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 选择action =========================</span>
</span></span><span class=line><span class=cl>        <span class=n>row</span> <span class=o>=</span> <span class=n>net</span><span class=p>(</span><span class=n>state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>random</span><span class=p>()</span> <span class=o>&lt;</span> <span class=n>epsilon</span><span class=p>:</span> <span class=c1># exploration</span>
</span></span><span class=line><span class=cl>            <span class=n>action</span> <span class=o>=</span> <span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>n_actions</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>action</span> <span class=o>=</span> <span class=n>row</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()</span><span class=o>.</span><span class=n>argmax</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 执行action =========================</span>
</span></span><span class=line><span class=cl>        <span class=n>state</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>terminated</span><span class=p>,</span> <span class=n>truncated</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 使用自定义的reward</span>
</span></span><span class=line><span class=cl>        <span class=n>reward</span> <span class=o>=</span> <span class=o>-</span><span class=mf>20.0</span> <span class=k>if</span> <span class=n>terminated</span> <span class=k>else</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>abs</span><span class=p>(</span><span class=n>state</span><span class=p>[</span><span class=mi>2</span><span class=p>])</span> <span class=o>&gt;</span> <span class=mf>0.1</span><span class=p>:</span> <span class=c1># 限制倾角</span>
</span></span><span class=line><span class=cl>            <span class=n>reward</span> <span class=o>+=</span> <span class=o>-</span><span class=mf>1.0</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>abs</span><span class=p>(</span><span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span> <span class=o>&gt;</span> <span class=mf>0.3</span><span class=p>:</span> <span class=c1># 限制位置</span>
</span></span><span class=line><span class=cl>            <span class=n>reward</span> <span class=o>+=</span> <span class=o>-</span><span class=mf>2.0</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>reward</span> <span class=o>&gt;=</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>reward</span> <span class=o>=</span> <span class=mf>1.0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 计算loss =========================</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>terminated</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>curr_q</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>reward</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>curr_q</span> <span class=o>=</span> <span class=n>net</span><span class=p>(</span><span class=n>state</span><span class=p>)</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>*</span> <span class=n>discount</span> <span class=o>+</span> <span class=n>reward</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>+=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>row</span><span class=p>[</span><span class=n>action</span><span class=p>],</span> <span class=n>curr_q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>terminated</span> <span class=ow>or</span> <span class=n>truncated</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>state</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 反向传播</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>loss</span><span class=o>/</span><span class=n>batch_size</span><span class=p>)</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>clip_grad_value_</span><span class=p>(</span><span class=n>net</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>env</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># 保存checkpoint</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>net</span><span class=o>.</span><span class=n>state_dict</span><span class=p>(),</span> <span class=s2>&#34;cartpole.ckpt&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>推理：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>env</span> <span class=o>=</span> <span class=n>gym</span><span class=o>.</span><span class=n>make</span><span class=p>(</span><span class=s2>&#34;CartPole-v1&#34;</span><span class=p>,</span> <span class=n>render_mode</span><span class=o>=</span><span class=s2>&#34;human&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>state</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>row</span> <span class=o>=</span> <span class=n>net</span><span class=p>(</span><span class=n>state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>action</span> <span class=o>=</span> <span class=n>row</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=n>row</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>state</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>terminated</span><span class=p>,</span> <span class=n>truncated</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>terminated</span> <span class=ow>or</span> <span class=n>truncated</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>state</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>env</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>不出意外的话，运行程序后可以看到类似于这样的动画，说明这个算法以及我们编写的程序都是有效的：
<img src=https://files.furffisite.link/blogimg/20230822103327-b82b26346196384f1796cefbd4ab8948-db19f.gif loading=lazy></p><p>完整的程序见<a class=link href=https://gist.github.com/Furffico/7ce3f2ef1dc0bc42536d2a178c5c5a92#file-cartpole-py target=_blank rel=noopener>Github Gist</a>，模型权重可以从<a class=link href=https://files.furffisite.link/files/cartpole.ckpt target=_blank rel=noopener>这里</a>下载（虽然自己训练一个也不费事）。</p><h2 id=经验回放>经验回放</h2><h3 id=理论-1>理论</h3><p>上面的训练 Q 网络的方式存在一些问题，例如</p><ol><li>样本的利用率低：每次采样只对应一次前向传播，采样得到的样本未被充分利用；</li><li>样本的时序关联性大：每次采样在时间上是高度相关的，上一次采样的末状态就是下一次采样的初始状态，影响训练效果；</li><li>训练速度慢：每次前向传播只传播一组数据，速度较慢。</li></ol><p>为了缓解上述问题，Mnih et al.<sup><a id=ref-cite2-2 href=#cite2>[2]</a></sup> 在提出深度 Q 学习的同时也提出了经验回放（Experience Replay）策略。
其主要思想是将采样与训练分离，采样时在记忆中保存采样的记录，训练时随机从记忆中选取样本进行前向与反向传播，从而降低样本间的时序关联性与提高样本利用率。</p><p>注意到在算法中，每一步训练需要四个值：当前状态 $s_t$、动作 $a_t$、回报 $r_t$ 以及采取动作后的状态 $s_{t+1}$，因此每一次采样后只需要在记忆中保存这四个值，称为 experience 四元组 $e_t=(s_t,~a_t,~r_t,~s_{t+1})$。</p><h3 id=实现-1>实现</h3><p>库、超参数、网络结构以及推理部分均沿用上面的代码以便比较，只替换训练部分，然后新增 Experience 类与 Memory 类用于存储和管理样本。
以下是 Experience 类与 Memory 类的代码，这里使用队列存储最新的 <code>batch_size*10</code> 条记录：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>NamedTuple</span><span class=p>,</span> <span class=n>Union</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Experience</span><span class=p>(</span><span class=n>NamedTuple</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;experience四元组&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>state</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>ndarray</span>
</span></span><span class=line><span class=cl>    <span class=n>action</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>    <span class=n>reward</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>    <span class=n>next_state</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>ndarray</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Memory</span><span class=p>(</span><span class=nb>object</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;存储固定数量记录的队列&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>buffer_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>buffer_size</span> <span class=o>=</span> <span class=n>buffer_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>buffer</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=n>Union</span><span class=p>[</span><span class=n>Experience</span><span class=p>,</span> <span class=kc>None</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>            <span class=o>=</span> <span class=p>[</span><span class=kc>None</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>buffer_size</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>count</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>append</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>exp</span><span class=p>:</span> <span class=n>Experience</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;增加记录，如果buffer已满则替换最早的记录&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>buffer</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>count</span><span class=o>%</span><span class=bp>self</span><span class=o>.</span><span class=n>buffer_size</span><span class=p>]</span> <span class=o>=</span> <span class=n>exp</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>count</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>sample</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>k</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;随机选取k个experience，打包好返回&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>count</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>buffer_size</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>pool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>buffer</span><span class=p>[:</span><span class=bp>self</span><span class=o>.</span><span class=n>count</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>pool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>buffer</span>
</span></span><span class=line><span class=cl>        <span class=n>exp</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=n>Experience</span><span class=p>]</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>choices</span><span class=p>(</span><span class=n>pool</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=n>k</span><span class=p>)</span> <span class=c1># type: ignore</span>
</span></span><span class=line><span class=cl>        <span class=c1># 打包成 Tensor</span>
</span></span><span class=line><span class=cl>        <span class=n>states</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>e</span><span class=o>.</span><span class=n>state</span> <span class=k>for</span> <span class=n>e</span> <span class=ow>in</span> <span class=n>exp</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>        <span class=n>actions</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>e</span><span class=o>.</span><span class=n>action</span> <span class=k>for</span> <span class=n>e</span> <span class=ow>in</span> <span class=n>exp</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>rewards</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>e</span><span class=o>.</span><span class=n>reward</span> <span class=k>for</span> <span class=n>e</span> <span class=ow>in</span> <span class=n>exp</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>next_states</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>e</span><span class=o>.</span><span class=n>next_state</span> <span class=k>for</span> <span class=n>e</span> <span class=ow>in</span> <span class=n>exp</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>states</span><span class=p>,</span> <span class=n>actions</span><span class=p>,</span> <span class=n>rewards</span><span class=p>,</span> <span class=n>next_states</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>memory</span> <span class=o>=</span> <span class=n>Memory</span><span class=p>(</span><span class=n>batch_size</span><span class=o>*</span><span class=mi>10</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>采样的部分与原来相同，而在训练的部分，因为这里训练时前向和反向传播都只有一步，所以在计算<code>target_q</code>时不需要像原文所述冻结权重，只要在其后增加<code>.detach()</code>确保反向传播时<code>target_q</code>的梯度不被传播就行。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>env</span> <span class=o>=</span> <span class=n>gym</span><span class=o>.</span><span class=n>make</span><span class=p>(</span><span class=s2>&#34;CartPole-v1&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>state</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>batch_index</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>batch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=n>tqdm</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 采样 =========================================</span>
</span></span><span class=line><span class=cl>    <span class=n>epsilon</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>t</span> <span class=o>/</span> <span class=n>epochs</span>  <span class=c1># 动态调整epsilon</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>batch_size</span><span class=o>//</span><span class=mi>4</span> <span class=k>if</span> <span class=n>t</span> <span class=o>&gt;=</span> <span class=mi>1</span> <span class=k>else</span> <span class=n>batch_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span> <span class=o>&lt;</span> <span class=n>epsilon</span><span class=p>:</span> <span class=c1># exploration</span>
</span></span><span class=line><span class=cl>            <span class=n>action</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>n_actions</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>action</span> <span class=o>=</span> <span class=n>net</span><span class=p>(</span><span class=n>state</span><span class=p>)</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()</span><span class=o>.</span><span class=n>argmax</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>org_state</span> <span class=o>=</span> <span class=n>state</span>
</span></span><span class=line><span class=cl>        <span class=n>state</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>terminated</span><span class=p>,</span> <span class=n>truncated</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 使用自定义的reward</span>
</span></span><span class=line><span class=cl>        <span class=n>reward</span> <span class=o>=</span> <span class=o>-</span><span class=mf>20.0</span> <span class=k>if</span> <span class=n>terminated</span> <span class=k>else</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>abs</span><span class=p>(</span><span class=n>state</span><span class=p>[</span><span class=mi>2</span><span class=p>])</span> <span class=o>&gt;</span> <span class=mf>0.1</span><span class=p>:</span> <span class=c1># 限制倾角</span>
</span></span><span class=line><span class=cl>            <span class=n>reward</span> <span class=o>+=</span> <span class=o>-</span><span class=mf>1.0</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>abs</span><span class=p>(</span><span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span> <span class=o>&gt;</span> <span class=mf>0.3</span><span class=p>:</span> <span class=c1># 限制位置</span>
</span></span><span class=line><span class=cl>            <span class=n>reward</span> <span class=o>+=</span> <span class=o>-</span><span class=mf>2.0</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>reward</span> <span class=o>&gt;=</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>reward</span> <span class=o>=</span> <span class=mf>1.0</span>
</span></span><span class=line><span class=cl>        <span class=c1># 加入记忆</span>
</span></span><span class=line><span class=cl>        <span class=n>memory</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>Experience</span><span class=p>(</span><span class=n>org_state</span><span class=p>,</span> <span class=n>action</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>state</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>terminated</span> <span class=ow>or</span> <span class=n>truncated</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>state</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 训练 =========================================</span>
</span></span><span class=line><span class=cl>    <span class=n>states</span><span class=p>,</span> <span class=n>actions</span><span class=p>,</span> <span class=n>rewards</span><span class=p>,</span> <span class=n>next_states</span> <span class=o>=</span> <span class=n>memory</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=n>batch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl>    <span class=n>pred_q</span> <span class=o>=</span> <span class=n>net</span><span class=p>(</span><span class=n>states</span><span class=p>)[</span><span class=n>batch_index</span><span class=p>,</span> <span class=n>actions</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>target_q</span> <span class=o>=</span> <span class=p>(</span><span class=n>net</span><span class=p>(</span><span class=n>next_states</span><span class=p>)</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>values</span> <span class=o>*</span> <span class=n>discount</span> <span class=o>+</span> <span class=n>rewards</span><span class=p>)</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>pred_q</span><span class=p>,</span> <span class=n>target_q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 反向传播</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>clip_grad_value_</span><span class=p>(</span><span class=n>net</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>env</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># 保存checkpoint</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>net</span><span class=o>.</span><span class=n>state_dict</span><span class=p>(),</span> <span class=s2>&#34;cartpole-replay.ckpt&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>完整程序见 <a class=link href=https://gist.github.com/Furffico/7ce3f2ef1dc0bc42536d2a178c5c5a92#file-cartpole-replay-py target=_blank rel=noopener>Github Gist</a>，运行程序，发现两个程序在<code>batch_size=128</code>和<code>epochs=5000</code>的情况下，原来的程序在我的轻薄本上需要训练 3 分钟，而得益于批处理的训练过程以及采样数的减少，有经验回放的训练只要 15 秒就能达到更好的效果。</p><p><img src=https://files.furffisite.link/blogimg/20230823103323-8b5eae7fdc273354106f2effa166ee3c-fc680.gif loading=lazy></p><p>继续增加<code>batch_size</code>或<code>epochs</code>，效果更佳。以下是<code>batch_size=256</code>、<code>epochs=5000</code>的结果，训练只花了 28 秒。</p><p><img src=https://files.furffisite.link/blogimg/20230823104647-a0f9ec917be67b838120250cf737771a-fcc8c.gif loading=lazy></p><h2 id=参考文献>参考文献</h2><style>.bibliography{display:table;font-size:medium;line-height:normal}.bib-item{display:table-row}.bib-item>:first-child{display:table-cell;padding-right:.5em;font-weight:700;text-align:right}.bib-item>:last-child{display:table-cell;padding-bottom:.5ex}</style><div class=bibliography><div id=cite1 class=bib-item><span>[1]</span>
<span>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning (arXiv:1312.5602). arXiv. <a class=link href=https://doi.org/10.48550/arXiv.1312.5602 target=_blank rel=noopener>https://doi.org/10.48550/arXiv.1312.5602</a><a href=#ref-cite1-1>⤶</a></span></div><div id=cite2 class=bib-item><span>[2]</span>
<span>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), Article 7540. <a class=link href=https://doi.org/10.1038/nature14236 target=_blank rel=noopener>https://doi.org/10.1038/nature14236</a><a href=#ref-cite2-1>⤶</a><a href=#ref-cite2-2>⤶</a></span></div></div></section><footer class=article-footer><section class=article-tags><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a>
<a href=/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>本文基于<a class=link href=https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode.zh-Hans target=_blank rel=noopener><u>CC BY-NC-SA 4.0 协议</u></a>发布。本文<strong>不可商用</strong>，转载时请<strong>注明出处</strong>并使用<strong>相同协议</strong>。</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/q-learning/><div class=article-image><img src=https://files.furffisite.link/blogimg/20230518191643-b6c8849bfb006d40abef7203bb5e15a0-18b9a.jpg loading=lazy data-key=Q-learning data-hash=https://files.furffisite.link/blogimg/20230518191643-b6c8849bfb006d40abef7203bb5e15a0-18b9a.jpg></div><div class=article-details><h2 class=article-title>【RL学习笔记】Q学习算法</h2></div></a></article><article class=has-image><a href=/p/read-papers/utsp/><div class=article-image><img src=https://files.furffisite.link/blogimg/20240312193735-df25ede3714c69d2ea8b42ba9c39f7c9-755f3.jpg loading=lazy data-key=read-papers/utsp data-hash=https://files.furffisite.link/blogimg/20240312193735-df25ede3714c69d2ea8b42ba9c39f7c9-755f3.jpg></div><div class=article-details><h2 class=article-title>【读论文】Unsupervised Learning for Solving the Travelling Salesman Problem</h2></div></a></article><article class=has-image><a href=/p/windows-setup/><div class=article-image><img src=https://files.furffisite.link/blogimg/20240707215654-5c276c9ae1c98340521bf56811f4b765-87c36.jpg loading=lazy data-key=windows-setup data-hash=https://files.furffisite.link/blogimg/20240707215654-5c276c9ae1c98340521bf56811f4b765-87c36.jpg></div><div class=article-details><h2 class=article-title>个人常用的新 Windows 11 系统的配置过程</h2></div></a></article><article class=has-image><a href=/p/applemusic-transfer/><div class=article-image><img src=https://files.furffisite.link/blogimg/20240523203808-9f4a7d937672d53fbc1a7174037c6f64-581fc.jpg loading=lazy data-key=applemusic-transfer data-hash=https://files.furffisite.link/blogimg/20240523203808-9f4a7d937672d53fbc1a7174037c6f64-581fc.jpg></div><div class=article-details><h2 class=article-title>Apple Music 曲库迁移</h2></div></a></article><article class=has-image><a href=/p/nas-4/><div class=article-image><img src=https://files.furffisite.link/blogimg/20240310022612-8671164f59f21e5d03de82e41e7073d5-7f2c7.jpg loading=lazy data-key=nas-4 data-hash=https://files.furffisite.link/blogimg/20240310022612-8671164f59f21e5d03de82e41e7073d5-7f2c7.jpg></div><div class=article-details><h2 class=article-title>我折腾 NAS 的历程（四）：一些升级</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=Furffico/Furffico.github.io data-repo-id=R_kgDOIs6c3Q data-category=General data-category-id=DIC_kwDOIs6c3c4CTWXf data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=en crossorigin=anonymous async></script><script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"light":"dark_dimmed")}})()</script><footer class=site-footer><section class=copyright>&copy;
2022 -
2024 Furffiblog</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.26.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://files.furffisite.link/fontcache/NotoSansSC_Mulish_remote.min.css",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>