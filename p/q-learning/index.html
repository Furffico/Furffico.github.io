<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Q-Learning Algorithm"><title>【RL学习笔记】Q学习算法</title>
<link rel=canonical href=https://blog.furffisite.link/p/q-learning/><link rel=stylesheet href=/scss/style.min.1d9acc6d7417da29e26c03e15654a8a97a1eb01eaece9c154c7b2b3721fec799.css><meta property='og:title' content="【RL学习笔记】Q学习算法"><meta property='og:description' content="Q-Learning Algorithm"><meta property='og:url' content='https://blog.furffisite.link/p/q-learning/'><meta property='og:site_name' content='Furffiblog'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='机器学习'><meta property='article:tag' content='强化学习'><meta property='article:published_time' content='2023-05-18T15:05:05+08:00'><meta property='article:modified_time' content='2023-05-18T15:05:05+08:00'><meta property='og:image' content='https://files.furffisite.link/blogimg/20230518191643-b6c8849bfb006d40abef7203bb5e15a0-18b9a.jpg'><meta name=twitter:title content="【RL学习笔记】Q学习算法"><meta name=twitter:description content="Q-Learning Algorithm"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://files.furffisite.link/blogimg/20230518191643-b6c8849bfb006d40abef7203bb5e15a0-18b9a.jpg'><link rel=preconnect href=https://files.furffisite.link crossorigin><link rel=preconnect href=https://gfonts.aby.pub crossorigin><script async src=https://analytics.umami.is/script.js data-website-id=b3257363-e219-4eac-b59d-d75b9aba64b5></script><link href=https://files.furffisite.link/fontcache/NotoSansSC_Mulish_remote.min.css rel=preload as=style><link href=https://gfonts.aby.pub/s/mulish/v13/1Ptyg83HX_SGhgqO0yLcmjzUAuWexZNR8aevGw.woff2 rel=preload as=font><style>:root{--zh-font-family:"Noto Sans SC", "PingFang SC", "Hiragino Sans GB", "Droid Sans Fallback", "Microsoft YaHei";--base-font-family:"Mulish", var(--sys-font-family), var(--zh-font-family), sans-serif}</style></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu8a7a5e24c6afd9e1fde1d549095c0023_459346_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🗑️</span></figure><div class=site-meta><h1 class=site-name><a href=/>Furffiblog</a></h1><h2 class=site-description>随便写写</h2></div></header><ol class=menu-social><li><a href=https://github.com/furffico target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li><a href=/gallery/><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32"><g fill="none"><path d="M3 7a4 4 0 014-4h14a4 4 0 014 4v14a4 4 0 01-4 4H7a4 4 0 01-4-4V7zm4-2A2 2 0 005 7v14a2 2 0 00.077.552l7.32-6.914c.9-.85 2.306-.85 3.205.0l7.32 6.914A2 2 0 0023 21V7a2 2 0 00-2-2H7zm0 18h14c.166.0.328-.02.482-.058l-7.253-6.85a.333.333.0 00-.458.0l-7.253 6.85c.155.038.316.058.482.058zm11.5-11a1.5 1.5.0 100-3 1.5 1.5.0 000 3zm-8 16.5a3.998 3.998.0 01-3.465-2H21a5.5 5.5.0 005.5-5.5V7.035c1.196.692 2 1.984 2 3.465V21A7.5 7.5.0 0121 28.5H10.5z" fill="currentcolor"/></g></svg>
<span>相册</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>文章归档</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#算法>算法</a></li><li><a href=#实现>实现</a><ol><li><a href=#frozen-lake>Frozen Lake</a></li><li><a href=#cliff-walking>Cliff Walking</a></li><li><a href=#taxi>Taxi</a></li><li><a href=#blackjack>Blackjack</a></li></ol></li><li><a href=#分析>分析</a><ol><li><a href=#优点>优点</a></li><li><a href=#局限性>局限性</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/q-learning/><img src=https://files.furffisite.link/blogimg/20230518191643-b6c8849bfb006d40abef7203bb5e15a0-18b9a.jpg loading=lazy alt="Featured image of post 【RL学习笔记】Q学习算法"></a></div><div class=article-details><header class=article-category><a href=/categories/%E7%AC%94%E8%AE%B0/>笔记</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/q-learning/>【RL学习笔记】Q学习算法</a></h2><h3 class=article-subtitle>Q-Learning Algorithm</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>May 18, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 3 分钟</time></div></footer></div></header><section class=article-content><h2 id=算法>算法</h2><p>Q 学习算法（Q-Learning Algorithm）的思路比较简单：使用 Q 函数记录<strong>每一个状态下每一个动作（action）的期望最大总回报（reward）</strong>，即 Q 值。在推理时贪心地选择当前状态 Q 值最大的动作，从而达到最大化期望总回报的目的。当问题的状态与动作均为离散时，Q 函数可以使用表格记录，这个表格也称为 Q 表格（Q-Table）。</p><p>设：</p><ul><li>问题的状态空间为 $S = {1,<del>2,</del>\ldots, m}$；</li><li>问题的动作空间为 $A = {1,<del>2,</del>\ldots, n}$；</li><li>探索阈值为 $\epsilon\in [0,1]$（推理时 $\epsilon = 0$）；</li><li>学习率 $\eta\in [0,1]$；</li><li>回报衰减率 $\gamma\in [0,1]$。</li></ul><p>则 Q 学习算法的流程如下：</p><ol><li>初始化 Q 表格为零矩阵 $Q=O_{m\times n}$；</li><li>设初始时间 $t = 0$，状态为 $s_t = s_0$；</li><li>选择动作 $a_t$：取随机数 $r\in[0,1]$，若 $r&lt;\epsilon$，则当前为探索阶段，从 $A$ 随机选取一个动作；否则 $a_t = \argmax_{j\in A} Q_{s_t, j}$；</li><li>与环境交互，执行动作 $a_t$，并获得状态 $s_{t+1}$、回报 $r_t$；</li><li>按照 <a class=link href=https://en.wikipedia.org/wiki/Bellman_equation target=_blank rel=noopener>Bellman 方程</a> 更新Q表格：
$$Q_{s_t, a_t} \leftarrow (1-\eta) Q_{s_t, a_t} + \eta(\gamma\max_{j\in A} Q_{s_{t+1},j} + r_t),$$
其中 $\max Q_{s_{t+1},j}$ 为 转移后的状态 $s_{t+1}$ 下最大的Q值，加上 $r_t$，组成转移前状态 $s_t$ 下 $a_t$ 操作的Q值。将其与原来的 $Q_{s_t, a_t}$ 加权平均就得到了更新后的Q值。</li><li>时间 $t\leftarrow t+1$，回到第 3 步。</li></ol><h2 id=实现>实现</h2><h3 id=frozen-lake>Frozen Lake</h3><p>以 <a class=link href=https://gymnasium.farama.org/ target=_blank rel=noopener>Gym</a> 的 <a class=link href=https://gymnasium.farama.org/environments/toy_text/frozen_lake/ target=_blank rel=noopener>Frozen Lake</a> 环境为例，其状态空间与动作空间都是离散的，因此适合使用 Q 表格。这个环境共有 $4^2=16$ 种状态，4 种动作，分别对应上下左右四个移动方向。环境中<code>slippery</code>引入的不确定性太大（每次行动只有$1/3$的概率能真正前往指定的方向），因此这里创建环境时<code>is_slippery</code>一项设为<code>False</code>。</p><p>实现如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>gymnasium</span> <span class=k>as</span> <span class=nn>gym</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>random</span> <span class=kn>import</span> <span class=n>random</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>env</span> <span class=o>=</span> <span class=n>gym</span><span class=o>.</span><span class=n>make</span><span class=p>(</span><span class=s1>&#39;FrozenLake-v1&#39;</span><span class=p>,</span> <span class=n>map_name</span><span class=o>=</span><span class=s2>&#34;4x4&#34;</span><span class=p>,</span> <span class=n>is_slippery</span> <span class=o>=</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>n_actions</span><span class=p>,</span> <span class=n>n_states</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>action_space</span><span class=o>.</span><span class=n>n</span><span class=p>,</span> <span class=n>env</span><span class=o>.</span><span class=n>observation_space</span><span class=o>.</span><span class=n>n</span>
</span></span><span class=line><span class=cl><span class=n>lr</span><span class=p>,</span> <span class=n>gamma</span> <span class=o>=</span> <span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.6</span>
</span></span><span class=line><span class=cl><span class=n>total</span> <span class=o>=</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>Q_table</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>n_states</span><span class=p>,</span> <span class=n>n_actions</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>state</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>total</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 选择动作</span>
</span></span><span class=line><span class=cl>    <span class=n>epsilon</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>t</span> <span class=o>/</span> <span class=n>total</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>random</span><span class=p>()</span> <span class=o>&lt;</span> <span class=n>epsilon</span><span class=p>:</span> <span class=c1># 探索</span>
</span></span><span class=line><span class=cl>        <span class=n>action</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>action_space</span><span class=o>.</span><span class=n>sample</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>action</span> <span class=o>=</span> <span class=n>Q_table</span><span class=p>[</span><span class=n>state</span><span class=p>]</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>org_state</span> <span class=o>=</span> <span class=n>state</span>
</span></span><span class=line><span class=cl>    <span class=n>state</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>terminated</span><span class=p>,</span> <span class=n>truncated</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>state</span> <span class=o>==</span> <span class=n>n_states</span><span class=o>-</span><span class=mi>1</span><span class=p>:</span> <span class=c1># 抵达终点，获得奖励</span>
</span></span><span class=line><span class=cl>        <span class=n>reward</span> <span class=o>=</span> <span class=mi>20</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>terminated</span><span class=p>:</span>        <span class=c1># 掉进洞里，给予惩罚</span>
</span></span><span class=line><span class=cl>        <span class=n>reward</span> <span class=o>=</span> <span class=o>-</span><span class=mi>5</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>                   <span class=c1># 让agent尽快抵达终点</span>
</span></span><span class=line><span class=cl>        <span class=n>reward</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 更新Q值</span>
</span></span><span class=line><span class=cl>    <span class=n>Q_table</span><span class=p>[</span><span class=n>org_state</span><span class=p>,</span> <span class=n>action</span><span class=p>]</span> <span class=o>=</span> <span class=p>((</span><span class=mi>1</span> <span class=o>-</span> <span class=n>lr</span><span class=p>)</span> <span class=o>*</span> <span class=n>Q_table</span><span class=p>[</span><span class=n>org_state</span><span class=p>,</span> <span class=n>action</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                                  <span class=o>+</span> <span class=n>lr</span> <span class=o>*</span> <span class=p>(</span><span class=n>reward</span> <span class=o>+</span> <span class=n>gamma</span> <span class=o>*</span> <span class=n>Q_table</span><span class=p>[</span><span class=n>state</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>terminated</span> <span class=ow>or</span> <span class=n>truncated</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>state</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>原环境提供的回报函数所含的信息较少（仅在抵达终点时为 1，其余情况均为 0），不利于算法的收敛，因此我在实现中重新设计了回报函数。
此外，训练时采用线性衰减的探索阈值 $\epsilon$ ，即训练初期倾向于探索（exploration），后期倾向于开发（exploitation）。</p><p>训练时每轮的 t 与该轮获得的总 reward 的折线图如下：</p><p><img src=https://files.furffisite.link/blogimg/20230518182030-919e874eac177ca3e790e7ee6f3046de-a01ac.png loading=lazy></p><p>推理时算法选择的路线如下图，确实是最优路线之一。</p><p><img src=https://files.furffisite.link/blogimg/20230518182047-803e364c89da138e7326b34667db82f2-057ca.gif loading=lazy></p><p>除了 Frozen Lake，这个方法也可以用来求解 Gym 提供的 <a class=link href=https://gymnasium.farama.org/environments/toy_text/cliff_walking/ target=_blank rel=noopener>Cliff Walking</a>、<a class=link href=https://gymnasium.farama.org/environments/toy_text/taxi/ target=_blank rel=noopener>Taxi</a> 与 <a class=link href=https://gymnasium.farama.org/environments/toy_text/blackjack/ target=_blank rel=noopener>Blackjack</a>。但是，随着问题规模的增大，训练步数也需要相应地增加。</p><h3 id=cliff-walking>Cliff Walking</h3><p>这个环境有 48 种状态和 4 种动作，因此 Q 表格内共有192项。以下是训练了 10,000 步的结果（平均每项 52 步）：</p><p><img src=https://files.furffisite.link/blogimg/20230518183755-c54395a25e87cd173c7c9b80b5e60d30-35491.gif loading=lazy></p><h3 id=taxi>Taxi</h3><p>这个环境有 500 种状态和 6 种动作，对应 Q 表格的 3000 项。以下是训练了 200,000 步的结果（平均每项 67 步）：</p><p><img src=https://files.furffisite.link/blogimg/20230518185029-6e6e54510ded0b9180a5d99c54ee3756-689cf.gif loading=lazy>
<img src=https://files.furffisite.link/blogimg/20230518214938-1806f73fe873209df4641d6007f123e8-43ce4.png loading=lazy></p><h3 id=blackjack>Blackjack</h3><p>Blackjack 有 $32\times 11\times 2=704$ 种状态和 $2$ 种动作，在处理时需要将离散的状态向量映射到非负整数域内。
这个游戏的状态转移存在不确定性，即使是充分训练（$5\times 10^6$ 步）的Q表格也只能将胜率从完全随机时的 28.2% 提升到 39.3%。</p><h2 id=分析>分析</h2><h3 id=优点>优点</h3><ul><li>算法简单，易于理解和实现。</li></ul><h3 id=局限性>局限性</h3><ul><li>基于 Q 表格的 Q 学习算法只能处理输入输出都是离散的问题；</li><li>基于 Q 表格的 Q 学习算法不能跨实例学习，即在遇到新的问题实例时，需要从 0 开始重新探索；</li><li>基于 Q 表格的 Q 学习算法难以处理训练过程中没有见过的状态；</li><li>当状态空间或动作空间很大时，Q 表格的规模也会很大，从而需要更长的学习时间。</li></ul><p>上述问题可以通过引入神经网络缓解，即深度 Q 学习算法（Deep Q-Learning Algorithm）。</p></section><footer class=article-footer><section class=article-tags><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a>
<a href=/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>本文基于<a class=link href=https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode.zh-Hans target=_blank rel=noopener><u>CC BY-NC-SA 4.0 协议</u></a>发布。本文<strong>不可商用</strong>，转载时请<strong>注明出处</strong>并使用<strong>相同协议</strong>。</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/deep-q-learning/><div class=article-image><img src=https://files.furffisite.link/blogimg/20230822110113-5ef18ab6cfb63d5a730bbd8e077231e7-6ba69.jpg loading=lazy data-key=deep-Q-learning data-hash=https://files.furffisite.link/blogimg/20230822110113-5ef18ab6cfb63d5a730bbd8e077231e7-6ba69.jpg></div><div class=article-details><h2 class=article-title>【RL学习笔记】深度Q学习算法与经验回放</h2></div></a></article><article class=has-image><a href=/p/read-papers/utsp/><div class=article-image><img src=https://files.furffisite.link/blogimg/20240312193735-df25ede3714c69d2ea8b42ba9c39f7c9-755f3.jpg loading=lazy data-key=read-papers/utsp data-hash=https://files.furffisite.link/blogimg/20240312193735-df25ede3714c69d2ea8b42ba9c39f7c9-755f3.jpg></div><div class=article-details><h2 class=article-title>【读论文】Unsupervised Learning for Solving the Travelling Salesman Problem</h2></div></a></article><article class=has-image><a href=/p/windows-setup/><div class=article-image><img src=https://files.furffisite.link/blogimg/20240707215654-5c276c9ae1c98340521bf56811f4b765-87c36.jpg loading=lazy data-key=windows-setup data-hash=https://files.furffisite.link/blogimg/20240707215654-5c276c9ae1c98340521bf56811f4b765-87c36.jpg></div><div class=article-details><h2 class=article-title>个人常用的新 Windows 11 系统的配置过程</h2></div></a></article><article class=has-image><a href=/p/applemusic-transfer/><div class=article-image><img src=https://files.furffisite.link/blogimg/20240523203808-9f4a7d937672d53fbc1a7174037c6f64-581fc.jpg loading=lazy data-key=applemusic-transfer data-hash=https://files.furffisite.link/blogimg/20240523203808-9f4a7d937672d53fbc1a7174037c6f64-581fc.jpg></div><div class=article-details><h2 class=article-title>Apple Music 曲库迁移</h2></div></a></article><article class=has-image><a href=/p/nas-4/><div class=article-image><img src=https://files.furffisite.link/blogimg/20240310022612-8671164f59f21e5d03de82e41e7073d5-7f2c7.jpg loading=lazy data-key=nas-4 data-hash=https://files.furffisite.link/blogimg/20240310022612-8671164f59f21e5d03de82e41e7073d5-7f2c7.jpg></div><div class=article-details><h2 class=article-title>我折腾 NAS 的历程（四）：一些升级</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=Furffico/Furffico.github.io data-repo-id=R_kgDOIs6c3Q data-category=General data-category-id=DIC_kwDOIs6c3c4CTWXf data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=en crossorigin=anonymous async></script><script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"light":"dark_dimmed")}})()</script><footer class=site-footer><section class=copyright>&copy;
2022 -
2024 Furffiblog</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.26.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://files.furffisite.link/fontcache/NotoSansSC_Mulish_remote.min.css",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>